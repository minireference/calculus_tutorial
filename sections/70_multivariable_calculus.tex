%!TEX root = ../calculus_tutorial.tex

\section{Multivariable calculus}
\label{sec:multivariable_calculus}

	In multivariable calculus,
	we extend the ideas of differential and integral calculus
	to functions with multiple input variables.
	If you understood the concepts of single-variable calculus,
	then you'll also be able to understand multivariable calculus:
	it's essentially the same stuff but in more dimensions!

	\subsection{Multivariable functions}

		A \emph{single-variable function} $f: \mathbb{R} \to \mathbb{R}$
		takes a real number $x \in \mathbb{R}$ as input
		and produces a real number $f(x) \in \mathbb{R}$ as output.
		A \emph{multivariable function} takes multiple real numbers as inputs.
		% and produces a real number as output.
		For example,
		a bivariate function $f: \mathbb{R} \times \mathbb{R} \to \mathbb{R}$
		takes two real numbers as inputs $(x,y) \in \mathbb{R} \times \mathbb{R}$
		and produces a real number $f(x,y) \in \mathbb{R}$ as output.

		% EXAMPLE FUNCTION
		Consider the bivariate function $f(x,y) = 4 - x^2 - \frac{1}{4}y^2$.
		% for all plots and calculations in the remainder of this section.
		We can plot this function as a \emph{surface} in a three-dimensional space,
		as shown in Figure~\ref{fig:mulivar_surface_plot_paraboloid}.
		The height of the surface above the point $(x,y)$ is function output $f(x,y)$.
		%	Suppose the height of a mountain is described by the function $f(x,y)$.
		%	The coordinates $(x,y)$ tell us the horizontal position point in the $xy$-plane
		%	and the value of the function $f(x,y)$ represents the height of the mountain at those coordinate.
		%	We identify the $z$ coordinate with the hight of the mountain $z=f(x,y)$
		%	and graph the function $f(x,y)$ is as a surface in 3D as illustrated in 

		\begin{figure}[htb]
			\centering
			\includegraphics[width=0.8\columnwidth]{figures/calculus/mulivar_surface_plot_paraboloid.pdf}
			\vspace{-2mm}
			\caption{The 3D surface plot of the function $f(x,y) = 4 - x^2 - \frac{1}{4}y^2$.}
			\label{fig:mulivar_surface_plot_paraboloid}
		\end{figure}

		\noindent			
		Surface plots are very good for visualizing multivariable functions,
		but they can be difficult to draw by hand.
		Another approach for representing the function $f(x,y)$
		is to use a two-dimensional plot that shows the ``view from above'' of the surface $f(x,y)$.
		We can trace \emph{level curves} in the surface,
		to produce a ``topographic map'' of the surface
		where each level curve shows the points that are at a certain height.
		% MAYBE: (i.e., the set of points $(x,y)$ satisfying $f(x,y)=c$ for some constant $c$
		%
		The curve labeled $0.0$ you see in Figure~\ref{fig:mulivar_countour_plot_paraboloid}
		represents the solution to the equation $f(x,y) = 0$,
		which is where the function $f(x,y)$ intersects the $xy$-plane.
		% where $f(x,y)$ is the height of this hill for all coordinates $(x,y)$ on the map.

		\begin{figure}[htb]
			\centering
			\includegraphics[width=0.8\columnwidth]{figures/calculus/mulivar_countour_plot_paraboloid.pdf}
			\vspace{-4mm}
			\caption{	Topographic map that shows the function $f(x,y)$ as level curves.}
			\label{fig:mulivar_countour_plot_paraboloid}
		\end{figure}


	\subsection{Partial derivatives}

		For a function of two variables $f(x,y)$,
		there is an ``$x$-derivative'' operator $\frac{\partial}{\partial x}$
		and a ``$y$-derivative'' operator $\frac{\partial}{\partial y}$.
		The operation $\frac{\partial}{\partial x}f(x,y)$ describes taking the derivative of $f(x,y)$ with respect to the input variable $x$,
		while keeping the input  variable $y$ constant.
		Taking the derivative of a multivariable function with respect to one of its input variables is called a \emph{partial derivative}
		and denoted with the symbol $\partial$.

		The partial derivative of $f(x,y)$ with respect to $x$ is
		\[
			\frac{\partial}{\partial x}f(x,y)
			\;\;=\;\;
			\frac{\partial f}{\partial x}
			\;\;\eqdef\;\;
			\lim_{ \delta \rightarrow 0}	\frac{f(x+\delta, y)-f(x,y)}{\delta}.
		\]
		Similarly the partial derivative of with respect to $y$ is
		\[
			\frac{\partial}{\partial y}f(x,y)
			\;\;=
			\frac{\partial f}{\partial y}
			\;\;\eqdef\;\;
			\lim_{ \delta \rightarrow 0}	\frac{f(x, y+\delta)-f(x,y)}{\delta}.
		\]
		Note that both $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$
		are functions of $x$ and $y$.

		Intuitively,
		the partial derivative $\frac{\partial f}{\partial x}$
		tells us the slope of the function $f(x,y)$ in the $x$-direction,
		and $\frac{\partial f}{\partial y}(x,y)$ tells us the slope of $f(x,y)$
		in the $y$-direction.

		%	Indeed,
		%	we can ask the questions ``what is the slope in the $x$-direction''
		%	and ``what is the slope in the '' at any point $(x,y)$ on the surface of the function.
		%	That's precisely the information returned by the functions $\frac{\partial f}{\partial x}(x,y)$ and .

		\subsubsection{Example}

			The partial derivatives of $f(x,y) = 4 - x^2 - \frac{1}{4}y^2$ are
			$\frac{\partial f}{\partial x} = -2x$ and $\frac{\partial f}{\partial y} = -\tfrac{1}{2}y$.
			%	\[
			%		\frac{\partial f}{\partial x} = -2x
			%		\qquad\text{and}\qquad
			%		\frac{\partial f}{\partial y} = -\tfrac{1}{2}y.
			%	\]

	\subsection{The gradient operator}

		The \emph{gradient} of the function
		is a vector that combines the $x$ and $y$ partial derivatives:
		\[
			\nabla f(x,y)
			\eqdef
			\left(
				\frac{\partial}{\partial x}f(x,y), \;
				\frac{\partial}{\partial y}f(x,y)
			\right)
			=
			\left(
				\frac{\partial f}{\partial x}, \;
				\frac{\partial f}{\partial y}
			\right).
		\]
		We use the symbol $\nabla$ (\emph{nabla}) for the gradient operation because it looks like an upside~$\Delta$,
		which is the symbol for change.
		% Note that $\nabla$ acts on a function $f(x,y)$ to produce a vector output.
		The direction of the gradient vector tells us the direction of the function's maximum increase---the
		``uphill'' direction at the surface of graph of $f(x,y)$ at the point $(x,y)$.
		The gradient vector is always perpendicular to the \emph{level curve} at that point.

		\subsubsection{Example}

			The gradient of the function $f(x,y) = 4 - x^2 - \frac{1}{4}y^2$ is
			$\nabla f(x,y)
					= \big(	\frac{\partial f}{\partial x},
							\frac{\partial f}{\partial y}	\big)
					= \big(	-2x,	 -\tfrac{1}{2}y	\big)$.
			%	\[
			%		\nabla f(x,y)
			%			= \left(	\frac{\partial f}{\partial x},
			%					\frac{\partial f}{\partial y}	\right)
			%			= \left(	-2x,	 -\tfrac{1}{2}y	\right).
			%	\]
			The gradient vector at coordinates  $(x,y) = (0,-4)$ is $\nabla f(0,-4) = (0,2)$,
			which is a vector pointing in the positive $y$-direction.
			Try to identify the point with coordinates $(0,-4)$
			in figures \ref{fig:mulivar_surface_plot_paraboloid} 
			and \ref{fig:mulivar_countour_plot_paraboloid}
			and confirm that the ``uphill'' direction at that point is indeed in the positive $y$-direction.


	\noindent
	The other half of multivariable calculus involves
	computing integrals of multivariable functions.


	\subsection{Partial integration}

		We can integrate over one of the input variables
		to produce a function that depends only on the other input variable:
		\[
			f(y) = \int f(x,y) \, dx
			\quad\text{and}\quad
			f(x) = \int f(x,y) \, dy.
		\]
		The functions $f(y)$ and $f(x)$ are called the \emph{partial integrals}
		or \emph{marginals} of the function $f(x,y)$.
		For example,
		the partial integrals of the function $f(x,y) = 4 - x^2 - \frac{1}{4}y^2$
		are obtained by computing the areas of ``slices'' throughout the function $f(x,y)$,
		as illustrated in Figure~\ref{fig:multivar_combined_slices_through_paraboloid}.

		\begin{figure}[htb]
			\centering
			\includegraphics[width=0.99\columnwidth]{figures/calculus/multivar_combined_slices_through_paraboloid.pdf}
			\vspace{-6mm}
			\caption{	Visualization of the partial integration procedures.}
			\label{fig:multivar_combined_slices_through_paraboloid}
		\end{figure}



	\subsection{Double integrals}

		The multivariable generalization of the integral $\int_a^b f(x) \, dx$
		that computes the ``total'' amount of $f(x)$ between $a$ and $b$
		is the multivariable integral of the form:
		\[
			\int \! \int_{(x,y) \in R} f(x,y) \, dxdy,
		\]
		where $R$ is called the \emph{region of integration}
		and corresponds to some subset of the Cartesian plane $\mathbb{R} \times \mathbb{R}$.
		The idea behind multivariable integrals is the same as for single variable integrals---to
		compute the total amount of some function accumulated over a range of input values.
		For single-variable functions,
		we integrate by splitting the region into thin rectangular strips of width $dx$.
		For double integrals,
		we split the two-dimensional region of integration into small squares of area $dxdy$,
		and compute the total volume of many rectangular columns
		with base $dxdy$ and height $f(x,y)$.

		% TODO: insert graphic of 3D integral split into vertical columns

		% TODO: explain "sweep along x then sweep along y" idea + hint at change-of-variables techniques

		%	% EXAMPLE		
		%	In the second part of multivariable calculus you'll learn how to do double integrals,
		%	which are integrals over two variables:
		%	\[
		%	  \int_{y=0}^{y=1}\! \int_{x=0}^{x=1} \!\! yx\;dxdy
		%	  	= \int_{y=0}^{y=1} \!\! \left[  y\frac{x^2}{2}\right]_{x=0}^{x=1} dy
		%	  	= \int_{y=0}^{y=1} \! \frac{y}{2}\, dy
		%		= \left[  \frac{1}{4}y^2\right]_{0}^{1}
		%		= \frac{1}{4}.
		%	\]
		%	Once you get over the initial shock of seeing two integral signs,
		%	you should be able to understand the above integral calculation.
		%	Proceeding from inside-out,
		%	a double integral is nothing more than two integral operators applied in succession.
		%	Instead of an region of integration split into tiny integration steps $dx$,
		%	we have a \emph{region of integration} split into tiny integration boxes with area $dxdy$.
		%	In the above example, the region of integration is a unit square in the $xy$-plane.



	\subsection{Applications of multivariable calculus}

		Multivariable functions appear all the time in machine learning, engineering, physics, and other sciences.
		% OPTIMIZATION
		The optimization techniques we discussed in
		Section~\ref{derivatives:optimization} (see page \pageref{derivatives:optimization})
		readily generally to functions with multiple variables.
		Indeed,
		the \emph{gradient descent algorithm} is often used to optimize functions
		with hundreds or thousands of variables.
		% Gradients are the key for the optimization algorithms used in modern machine learning techniques.
		Many of the cutting-edge machine learning models
		``learn'' the model parameters by minimizing the value of some multivariable function
		and repeatedly taking steps in the ``downhill'' direction,
		as indicated by the gradient vector.
		% MULTIVAR INTEGRALS 
		Multivariable integrals are used a lot in probability theory and statistics,
		where they are used to compute probabilities and expectations.			
		%	There is a lot more to learn about calculus operations on multivariable functions,
		%	but we don't have the space for further discussion.
		Your basic knowledge of derivatives and integrals concepts we discussed
		earlier in this tutorial will be very useful for understanding multivariable calculus topics.




%	You'll encounter the gradient descent algorithm and its numerous variations
%	if you choose to purse the topic of machine learning,
%	since they are used in many machine learning applications.	% where multivariable functions are the norm

% The notion of an uphill or downhill direction for the surface $f(x,y)$ turns out to be very useful for optimization.
	%	If you want to find the local maximum of a function,
	%	you can start at some point and keep moving uphill (in the direction of $\nabla f(x,y)$ and you'll arrive at a local peak of the mountain.
	%	Similarly,
%	To find the lowest point on the surface (minimum value of $f(x,y)$),
%	you can start at some point and keep moving downhill,
%	that is in the opposite direction to the gradient $-\nabla f(x,y)$.
% Figure~\ref{fig:multivariable_caclulus_topographic_map_waterflow_to_bottom} illustrates this process.
% Consider the path of a water stream whose source in some arbitrary point on the surface of the mountain.
%	Intuitively,
%	this is the path that a water stream would take as it descends down the slope of the mountain until it reaches the minimum at the bottom of a valley.
%	This intuitive notion of ``keep moving downhill until you get to a local minimum'' is
%	the general idea behind the \emph{gradient descent} optimization algorithm which is very important for machine learning applications.

%	\begin{figure}[htb]
%	\centering
%	\includegraphics[width=0.63212\columnwidth]{figures/calculus/multivariable_caclulus_topographic_map_waterflow_to_bottom.png}
%	\vspace{-4mm}
%	\caption{This graph shows the path taken by a hypothetical water as it flows to the bottom of the valley $f(x,y)$.}
%	\label{fig:multivariable_caclulus_topographic_map_waterflow_to_bottom}
%	\end{figure}

%	We know we've reached the bottom of the valley,
%	since the gradient vector will be zero at the minimum of the function $f(x,y)$,
%	since surface is locally flat there.

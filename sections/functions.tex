%!TEX root = ../calculus_tutorial.tex


\section{Functions}

	A \emphindexdef{function} is a mathematical object that takes numbers as inputs and produces numbers as outputs.
	We use the notation
	\[
		f \colon A \to B
	\]
	to denote a function from the input set $A$ to the output set $B$.
	For every input $x$, the output value of $f$ for that input is denoted $f(x)$.

	\subsection{Function graph}

		The \emph{graph} of a function is a line that passes through all input-output pairs of a function.
		Imagine we take out a piece of paper and draw a coordinate system with a horizontal axis and a vertical axis.
		The horizontal axis describes the different input values $x$,
		while the vertical axis describes the output values $f(x)$.
		Each input-output pair of the function $f$ corresponds to the point $(x,f(x))$ in the coordinate system.
		We obtain the graph of the function by varying the input coordinate $x$ and plotting all the points $(x, f(x))$,
		as illustrated in Figure~\ref{fig:graph_of_f_nonnegative}.

		\begin{figure}[htb]
			\centering
			\includegraphics[width=0.3\textwidth]{figures/calculus/graph_of_f_nonnegative.pdf}
% REPLACE WITH original (x, 0.5 x^2)
			\caption{	The graph of the function $f$ consists of all the points with coordinates $(x,f(x))$ over some interval of $x$ values.}
			\label{fig:graph_of_f_nonnegative}
		\end{figure}

		The graph of the function $f$ allows us to see at a glance the behaviour of the function for all possible inputs,
		and forms an essential visualization tool.
		Indeed,
		many phenomena and calculations related to functions
		can be understood geometrically as operations based on the graph of the function.

		In probability theory,
		we use functions to describe the probability distributions of random variables.
		Discrete random variables are described by a probability mass function of the form $f\colon \mathcal{X} \to \mathbb{R}$,
		where the sample space $\mathcal{X}$ is either a finite set or a countably infinite set like the natural numbers $\mathbb{N}$.
		Continuous random variables are described by probability density functions of the form 
		$f\colon \mathcal{X} \to \mathbb{R}$,
		where the sample space $\mathcal{X}$ is some subset of the real numbers $\mathbb{R}$.


	\subsection{Plotting function graphs using NumPy and Seaborn}

		
		We can use a combination of the \tt{numpy} and \tt{seaborn} modules
		to plot the graph of the function $g(x) = \frac{1}{2}x^2$,
		as shown in the code example below.

		\begin{codeblock}[plot-gx-0-10]
		>>> def g(x):
		        return 0.5 * x**2
		>>> import numpy as np
		>>> import seaborn as sns
		>>> xs = np.linspace(0, 10, 1000)
		>>> gxs = g(xs)
		>>> sns.lineplot(x=xs, y=gxs, label="Graph of g(x)")
		See Figure ยก\ref{fig:graph_of_function_g_eq_halfx2}ยก for the output.
		\end{codeblock}

		\noindent
		We import the module \tt{numpy} under the alias \tt{np}.
		We use the function \tt{np.linspace} to create an array (a list of numbers) \tt{xs},
		which contains 1000 input values that range from $x=0$ until $x=10$.
		Next we apply the function $g$ to the array of inputs \tt{xs} and store the result in the array \tt{gxs},
		which contains all the output values of the function for the input values \tt{xs}.
		At this point,
		the arrays \tt{xs} and \tt{gxs} contain $1000$ input-output pairs of the form $(x, g(x))$,
		which is exactly what we need to plot the graph of the function.
		On the last line,
		we call the function \tt{lineplot} to create the graph of $g(x)$,
		which produces the plot shown in Figure~\ref{fig:graph_of_function_g_eq_halfx2}.

		\begin{figure}[htb]
			\centering
			\includegraphics[width=0.4\textwidth]{figures/graph_of_function_g_eq_halfx2.pdf}
			\vspace{-2mm}
			\caption{	Graph of the function $g(x)=x$ from $x=0$ until $x=10$.}
			\label{fig:graph_of_function_g_eq_halfx2}
\vspace{-4mm}
		\end{figure}

%			Note the steps we used to obtain the function graph in code~\ref{plot-gx-0-10}
%			correspond exactly to the mathematical procedure for drawing the graph of $f(x)$:
%			draw the line that passes through all $(x, f(x))$ input-output pairs.







	\subsection{Inverse functions}

		The inverse function $f^{-1} \colon B \to A$ performs the \emph{inverse operation} of the function $f \colon A \to B$.
		If you start from some $x$, apply $f$, and then apply $f^{-1}$,
		you'll arrive---full circle---back to the original input $x$:
		\[
			f^{-1}\!\big( \; f(x) \; \big) = x.
		\]
		In Figure~\ref{fig:functions-inverse} the function $f$ is represented as a forward arrow,
		and the inverse function $f^{-1}$ is represented as a backward arrow
		that puts the value $f(x)$ back to the $x$ it came from.

		\begin{figure}[htb]
			\centering
			\includegraphics[width=0.25\textwidth]{figures/calculus/functions-inverse.pdf}
			\caption{The inverse $f^{-1}$ undoes the operation of the function $f$.}
			\label{fig:functions-inverse}
		\end{figure}

		For example,
		if we compute the square root of a number, then square the result,
		we obtain the original number,
		since the quadratic function $x^2$ is the inverse of the square-root function $\sqrt{x}$.

		\begin{codeblock}[fun-inv-fun-combo-sqrt]
		>>> np.sqrt(4)**2
		4.0
		\end{codeblock}

		\noindent
		The exponential function $e^x$ is the inverse of the logarithmic function $\log_e(x)$,
		so if we compute the logarithm of a number then apply the exponential function,
		we get back the original input.

		\begin{codeblock}[fun-inv-fun-combo-log]
		>>> np.exp(np.log(4))
		4.0
		\end{codeblock}

		\noindent
		In probability theory,
		we often do calculations using the cumulative distribution function (CDF) $F_X \colon \calX \to [0,1]$,
		and also use the inverse of the cumulative distribution function $F_X^{-1}\colon [0,1] \to \calX$.
		Knowing about inverse functions (and the weird superscript $^{-1}$ notation used to describe them)
		is useful for your conceptual understanding of these concepts:
		instead of thinking about the inverse-CDF $F_X^{-1}$ as some new complicated concept you have to memorize,
		you can think of $F_X^{-1}$ as the ``undo operation'' for $F_X$.
		In other words,
		$F_X$ and $F_X^{-1}$ describe the same mapping,
		but used in opposite directions.

		%		$F_X(b) = q_b$
		%		vs. $F_X^{-1}(q) = b_q$  how far in the sample space do I have to go to encompass proportion $q$ of the total probability.

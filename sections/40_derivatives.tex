%!TEX root = ../calculus_tutorial.tex

\section{Derivatives}
\label{sec:derivatives}

	The \emph{derivative} function, denoted $f^{\prime\!}(x)$, $\frac{d}{dx}f(x)$, or $\frac{df}{dx}$,
	describes the \emph{rate of change} of the function $f(x)$.
	For example,
	the constant function $f(x)=c$ has derivative $f^{\prime\!}(x)=0$ since the constant function doesn't change.
	Geometrically,
	the derivative function describes the \emph{slope} of the graph of the function $f(x)$.
	The derivative of the line $f(x)=mx+b$ is $f^{\prime\!}(x)=m$,
	since the slope of this line is equal to $m$
	for all values of $x$.
	In general,
	the slope of a function is different at different values of~$x$,
	so mathematicians invented the notation $f^{\prime\!}(x)$
	for describing ``the slope of the function $f$ at $x$.''


% NOD
Consider the rise-over-run formula for calculating the slope of a function
applied to the points $(x,f(x))$ and $(x+\delta, f(x+\delta))$ on the graph of the function.
Figure~REF
shows the slopes calculated when $\delta = 0.1$, $\delta = 0.01$, and $\delta = 0.001$.


The average slope between ... and ...
is

TODO: figure

but what about the instantaneous slope at one instant?



	The derivative function $f^{\prime\!}(x)$
	is defined as the following limit expression:
	\[
		f^{\prime\!}(x) \eqdef \lim_{\delta \to 0} \frac{f(x+\delta)\ - \ f(x)}{\delta}\,.
	\]
	In words,
	this formula describes the general rise-over-run calculation for computing the slope of a line that connects
	the points $(x,f(x))$ and $(x+\delta, f(x+\delta))$,
	with the step-length $\delta$ becoming infinitely small.

	%	The definition of the derivative comes from the rise-over-run formula for calculating the slope of a line:
	%	\[
	%	  \frac{ \textrm{rise} } { \textrm{run} } = \frac{ \Delta y } { \Delta x } 
	%		=  \frac{y_f - y_i}{x_f - x_i} = 
	%		\frac{f(x+\delta)\ - \ f(x)}{x + \delta \  -\  x}.
	%	\]
	%	By making $\delta$ tend to zero in the above expression,
	%	we're performing a rise-over-run of the function $f(x)$ at a point.

	Geometrically,
	the derivative function tells us the slope of the graph of the function $f(x)$ for all values of $x$.
	Figure~\ref{fig:derivative_as_slope_xsq} shows the slope calculation for the function $f(x) = x^2$
	at two different locations: at $x=-0.5$ and at $x=1$.
	% the slope of the function is the same as the line passing through this point


	\begin{figure}[htb]
		\centering
		\includegraphics[width=0.99\columnwidth]{figures/calculus/derivative_as_slope_xsq.pdf}
		\caption{	The derivative of the function at $x=a$ is denoted $f^{\prime\!}(a)$ and describes the slope function at that point.}
		%	\caption{	The diagram illustrates how to compute the derivative of the function $f(x) = x^2$
		%			at three different points on the graph of the function.
		%			To calculate the derivative of $f(x)$ at $x=1$, 
		%			we can ``zoom in'' near the point $(1,\tfrac{1}{2})$ and 
		%			draw a line that has the same slope as the function.
		%	 	 	We can then calculate the slope of the line using a rise-over-run calculation,
		%			aided by the mini coordinate system that is provided.
		%			The derivative calculations for $x=-\tfrac{1}{2}$ and  $x=2$ are also shown.
		%			Note that the slope of the function is different for each value of $x$. 
		%			What is the value of the derivative at $x=0$?
		%			Can you find the general pattern?}
		\label{fig:derivative_as_slope_xsq}
	\end{figure}

% TODO: explain grid 

	%	Derivatives occur so often in math that people have devised many ways to denote them:
	%	\[
	%	    Df(x) =  f^{\prime\!}(x) =  \frac{d}{dx}f(x) = \frac{df}{dx} = \frac{dy}{dx} = \nabla f.
	%	\]
	%	Don't be fooled by this multitude of notations---all of them refer to the same concept.

	% TODO: mention the derivative is a function of the form $f': \mathbb{R} \to \mathbb{R}$.
	The derivative function $f^{\prime\!}(x)$ is a property of the function $f(x)$.
	Indeed, this is where the name \emph{derivative} comes from:
	$f^{\prime\!}(x)$ is not an independent function---it
	is \emph{derived} from the slope property of the original function $f(x)$.

	More generally,
	we can define the \emph{derivative operation},
	denoted $\frac{d}{dx}[\tt{<f>}]$,		% ALT. derivative operator $\frac{d}{dx}$
	which takes as input a function $f(x)$ and produces as output the derivative function $f^{\prime\!}(x)$.
	% which is a function of the form $f': \mathbb{R} \to \mathbb{R}$.
	% For each input $x_0$ the derivative function tells you the slope of $f(x)$ when $x=x_0$.
	Applying the derivative operation to the function
	is also called ``taking the derivative'' of a function.
	For example, 
	the derivative of the function $f(x) = x^2$ is the function $f^{\prime\!}(x) = 2x$.
	We can also describe this relationship as $(x^2)^{\prime} = 2x$
	or as $\tfrac{d}{dx}(x^2) = 2x$.
	Look at the graph in Figure~\ref{fig:derivative_as_slope_xsq} to convince yourself
	that the slope of $f(x)=x^2$ is indeed described by $f^{\prime\!}(x)=2x$ for any $x$.
	For example,
	when $x=0$,
	we see the graph has zero slope
	and the derivative gives us the same thing: $f^{\prime\!}(0) = 0$.


	\subsection{Numerical derivative calculations}

		Here is the Python code for computing
		a numerical approximations to the derivative the function \tt{f} at the point~\tt{x}:

		\begin{codeblock}[]
		>>> def differentiate(f, x, delta=1e-9):
		        df = f(x+delta) - f(x)
		        dx = delta
		        return df / dx
		\end{codeblock}

		\noindent
		The function \tt{differentiate} calculates the derivative using a finite step $\tt{delta} = 10^{-9}$
		instead of the infinitely small step $\delta$ in the math definition of the derivative.
		This means,
		the value returned by \tt{differentiate} will be an approximation to the true derivative.

		Let's now define a Python function \tt{f} that corresponds to the math function $f(x)=x^2$
		and use \tt{differentiate} to find the slope of $f$ when $x=1$:

		\begin{codeblock}[]
		>>> def f(x):
		        return x**2
		>>> differentiate(f, 1)
		2.000000165480742
		\end{codeblock}
		
		\noindent
		Using the numerical method,
		we obtain the approximation $f^{\prime\!}(1) = 2.000000165480742$,
		which is not perfect,
		but pretty close to the true value $f^{\prime\!}(1) = 2$.
		For most practical applications,
		this numerical approximation is good enough.

		%	The mathematical definition
		%	$f^{\prime\!}(x) \eqdef \lim_{ \delta \rightarrow 0}\frac{f(x+\delta)-f(x)}{\delta}$
		%	allows us to find a closed form expression for the derivative function
		%	that applies for all values of $x$.
		%	Finding the exact formula for the derivative requires a little bit more work upfront
		%	(to simplify an expressions that involves Greek symbols),
		%	but once you find the exact formula for the derivative $f^{\prime\!}(x)$,
		%	you can compute the slope of $f(x)$ at the point $x=c$
		%	by simply evaluating the the derivative function at that point:
		%	$f^{\prime\!}(c) = \{ \textrm{the slope of} \ f(x) \ \textrm{at} \ x=c\}$.



	\subsection{Derivative formulas}

		You don't need to apply the complicated derivative formula
		$f^{\prime\!}(x) \eqdef \lim_{\delta \to 0}\frac{f(x+\delta)-f(x)}{\delta}$
		every time you need to find the derivative of a function.
		For each function $f(x)$,
		it's enough to use the complicated formula once
		and record the formula you obtain for $f^{\prime\!}(x)$,
		then you can reuse that formula
		whenever you need to compute $f^{\prime\!}(x)$ in later calculations.

		Table~\ref{table:derivatives} shows the derivatives several functions.
		I invite you to mentally bookmark this page so you can come back to it
		when you need to know the derivatives of some function.

		\begin{table}[htb]
			\centering
			\caption{Derivative formulas for commonly used functions}
			\begin{shadebox}%
			\vspace{-2mm}
			\begin{align*}
			f(x)					&  \ -\textrm{derivative}\to  \   											f^{\prime\!}(x)		\\
			a					&\qquad  \raisebox{.52ex}{\rule{0.9em}{.4pt}}\;\tfrac{d}{dx} \rightarrow \qquad 		0		\\
			x					&\qquad  \raisebox{.52ex}{\rule{0.9em}{.4pt}}\;\tfrac{d}{dx} \rightarrow \qquad		1		\\
			mx+b				&\qquad  \raisebox{.52ex}{\rule{0.9em}{.4pt}}\;\tfrac{d}{dx} \rightarrow \qquad		m		\\
			x^n, \text{ for } n\neq 0	&\qquad  \raisebox{.52ex}{\rule{0.9em}{.4pt}}\;\tfrac{d}{dx} \rightarrow \qquad 		nx^{n-1}	\\
			\tfrac{1}{x} =  x^{-1}		&\qquad  \raisebox{.52ex}{\rule{0.9em}{.4pt}}\;\tfrac{d}{dx} \rightarrow \qquad 		\tfrac{-1}{x^2} = -x^{-2}	\\
			\sqrt{x} = x^{\frac{1}{2}}	&\qquad  \raisebox{.52ex}{\rule{0.9em}{.4pt}}\;\tfrac{d}{dx} \rightarrow \qquad
					\tfrac{1}{2\sqrt{x}} = \tfrac{1}{2}x^{-\frac{1}{2}}	\\
			e^x					&\qquad  \raisebox{.52ex}{\rule{0.9em}{.4pt}}\;\tfrac{d}{dx} \rightarrow \qquad 		e^x	\\
			\ln(x)					&\qquad  \raisebox{.52ex}{\rule{0.9em}{.4pt}}\;\tfrac{d}{dx} \rightarrow \qquad 		\tfrac{1}{x}		\\
			\sin(x)				&\qquad  \raisebox{.52ex}{\rule{0.9em}{.4pt}}\;\tfrac{d}{dx} \rightarrow \qquad 		\cos(x)		\\
			\cos(x)				&\qquad  \raisebox{.52ex}{\rule{0.9em}{.4pt}}\;\tfrac{d}{dx} \rightarrow \qquad 		-\sin(x)
			\end{align*}%
			\end{shadebox}
			\label{table:derivatives}
		\end{table}
%		\alpha f(x)+ \beta g(x)		&\qquad  \raisebox{.52ex}{\rule{0.9em}{.4pt}}\;\tfrac{d}{dx} \rightarrow \qquad 	\alpha f^{\prime\!}(x)+ \beta g^{\prime\!}(x)	\\
%	af(x)			&\qquad  \raisebox{.52ex}{\rule{0.9em}{.4pt}}\;\tfrac{d}{dx} \rightarrow \qquad 				af^{\prime\!}(x)		\\
%	f(x)+g(x)		&\qquad  \raisebox{.52ex}{\rule{0.9em}{.4pt}}\;\tfrac{d}{dx} \rightarrow \qquad 				f^{\prime\!}(x)+g^{\prime\!}(x)	\\
% a^x			&\qquad  \raisebox{.52ex}{\rule{0.9em}{.4pt}}\;\tfrac{d}{dx} \rightarrow \qquad 				a^x\ln(a)	\\
% \log_a(x)		&\qquad  \raisebox{.52ex}{\rule{0.9em}{.4pt}}\;\tfrac{d}{dx} \rightarrow \qquad 				(x\ln(a))^{-1}	\\
% \tan(x)			&\qquad  \raisebox{.52ex}{\rule{0.9em}{.4pt}}\;\tfrac{d}{dx} \rightarrow \qquad 				\sec^2(x) = \cos^{-2}(x)
%	\csc(x) = \frac{1}{\sin(x)}		&\qquad  \raisebox{.52ex}{\rule{0.9em}{.4pt}}\;\tfrac{d}{dx} \rightarrow \qquad 	-\sin^{-2}(x)\cos(x)	\\
%	\sec(x) = \frac{1}{\cos(x)}		&\qquad  \raisebox{.52ex}{\rule{0.9em}{.4pt}}\;\tfrac{d}{dx} \rightarrow \qquad 	\tan(x)\sec(x)	\\
%	\cot(x) = \frac{1}{\tan(x)}		&\qquad  \raisebox{.52ex}{\rule{0.9em}{.4pt}}\;\tfrac{d}{dx} \rightarrow \qquad 	-\csc^2(x)	\\
%	\sin^{-1}(x)		&\qquad  \raisebox{.52ex}{\rule{0.9em}{.4pt}}\;\tfrac{d}{dx} \rightarrow \qquad 				\frac{1}{\sqrt{1-x^2}}	\\
%	\cos^{-1}(x)		&\qquad  \raisebox{.52ex}{\rule{0.9em}{.4pt}}\;\tfrac{d}{dx} \rightarrow \qquad 				\frac{-1}{\sqrt{1-x^2}}	\\
%	\tan^{-1}(x)		&\qquad  \raisebox{.52ex}{\rule{0.9em}{.4pt}}\;\tfrac{d}{dx} \rightarrow \qquad 				\frac{1}{1+x^2}	\\
%	\sinh(x)		&\qquad  \raisebox{.52ex}{\rule{0.9em}{.4pt}}\;\tfrac{d}{dx} \rightarrow \qquad 				\cosh(x)	\\
%	\cosh(x)		&\qquad  \raisebox{.52ex}{\rule{0.9em}{.4pt}}\;\tfrac{d}{dx} \rightarrow \qquad 				\sinh(x)	

		\noindent
		Table~\ref{table:derivatives} presents the results
		in terms of the derivative operator $\frac{d}{dx}[\tt{<f>}]$,
		which takes as input some function $f(x)$
		and produces as output its derivative function $f^{\prime\!}(x)$.


	\subsection{Derivative rules}

		In addition to the table of derivative formulas,
		there are some important derivatives rules
		that allow you to find derivatives of \emph{composite} functions.

		\paragraph{Constant multiple rule}

			The derivative of $k$ times the function $f(x)$
			is equal to $k$ times the derivative of $f(x)$:
			\[
				\big[ k f(x) \big]^\prime		= 	k f^{\prime\!}(x).
			\]

		\paragraph{Sum rule}

			The derivative of the sum of two functions
			is the sum of their derivatives:
			\[
				\big[ f(x) +  g(x) \big]^\prime	=	f^{\prime\!}(x) + g^{\prime\!}(x).
			\]

			%	The derivative of a \emph{linear combination} of functions $\alpha f(x) + \beta g(x)$ is equal 
			%	to the same linear combination of the derivatives $\alpha f^{\prime\!}(x) + \beta g^{\prime\!}(x)$.


		\paragraph{Product rule}

			The derivative of a product of two functions
			is the sum of two contributions:
			\[
				\big[ f(x)g(x) \big]^\prime 		= 	f^{\prime\!}(x)g(x)  + f(x)g^{\prime\!}(x).
			\]
			In each term,
			the derivative of one of the functions
			is multiplied by the value of the other function.


		\paragraph{Quotient rule}

			This formula tells us how to obtain the derivative of a fraction of two functions:
			\[
				\left[ \frac{f(x)}{g(x)} \right]^\prime =	\frac{f^{\prime\!}(x)g(x)-f(x)g^{\prime\!}(x)}{g(x)^2}.
			\]
			
			
		\paragraph{Chain rule}
			If you encounter a situation that includes an inner function and an outer function,
			like $f(g(x))$, you can obtain the derivative by a two-step process:
			\[
				\big[ f(g(x)) \big]^\prime	= 	f^{\prime\!}(g(x))g^{\prime\!}(x).
			\]
			
			\noindent
			In the first step,
			we leave the inner function $g(x)$ alone
			and focus on taking the derivative of the outer function $f(x)$.
			This step gives us $f^{\prime\!}(g(x))$,
			which is the value of $f^{\prime}$ evaluated at $g(x)$.
			In the second step,
			we multiply this expression by the derivative of the \emph{inner} function $g^{\prime\!}(x)$.


	\subsection{Higher derivatives}

		The second derivative of $f(x)$ is denoted $f^{\prime\prime\!}(x)$ or $\frac{d^2f}{dx^2}$.
		It is obtained by applying the derivative operation to $f(x)$ \emph{twice}:
		$\frac{d}{dx}\big[ \frac{d}{dx}[\tt{<f>}] \big]$.
		Geometrically,
		the second derivative $f^{\prime\prime\!}(x)$
		tells us the \emph{curvature} of $f(x)$.
		Positive curvature means the function opens upward
		and looks like the bottom of a valley.
		The function $f(x)=x^2$ shown in Figure~\ref{fig:derivative_as_slope_xsq} 
		has derivative $f^{\prime\!}(x) = 2x$ and second derivative $f^{\prime\prime\!}(x) = 2$,
		which means it has positive curvature.
		Negative curvature means the function opens downward
		and looks like a mountain peak.
		For example,
		the function $g(x) = -x^2$ has negative curvature.


	\subsection{Examples}
	
		Armed with the derivative formulas from Table~\ref{table:derivatives} and the derivative rules from the previous section,
		you can the derivative of any function,
		no matter how complicated.
		Let's look at some examples.

		\paragraph{Example 1}
			To calculate the derivative of $f(x) = e^{x^2}$,
			we use the chain rule: $f^{\prime\!}(x) = e^{x^2}[x^2]'  = e^{x^2}2x$.

		\paragraph{Example 2}
			To find the derivative of $f(x) = \sin(x)e^{x^2}$,
			we use the product rule and the chain rule: $f^{\prime\!}(x) = \cos(x)e^{x^2} + \sin(x)2xe^{x^2}$.
	
		\paragraph{Example 3}
			The derivative of $\sin(x^2)$ requires using the chain rule:
			$\left[ \sin(x^2) \right]^\prime =  \cos(x^2)\left[x^2\right]' =  \cos(x^2)2x$.




	\subsection{Computing derivatives using SymPy}

		The \texttt{SymPy} function \texttt{sp.diff} computes the derivative of any expression.
		For example,
		here is how to compute the derivative of the function $f(x) = mx +b$:

		\begin{codeblock}[sympy-diff-line]
		>>> m, x, b = sp.symbols("m x b")
		>>> sp.diff(m*x + b, x)
		m
		\end{codeblock}

		\noindent		
		Let's also verify the derivative formula $\tfrac{d}{dx}[x^n] = nx^{n-1}$:

		\begin{codeblock}[]
		>>> x, n = sp.symbols("x n")
		>>> sp.diff(x**n, x)
		n * x**(n - 1)
		\end{codeblock}

		\noindent
		The exponential function $f(x)=e^x$ is special
		because it is the only function that is equal to its derivative:

		\begin{codeblock}[]
		>>> from sympy import exp
		>>> sp.diff(exp(x), x)
		exp(x)
		\end{codeblock}

		\noindent
		Let's use SymPy to check the derivative calculations from the examples:

		\begin{codeblock}[]
		>>> sp.diff(sp.exp(x**2), x)
		2*x*exp(x**2)
		>>> sp.diff(sp.sin(x)*sp.exp(x**2), x)
		2*x*exp(x**2)*sin(x) + exp(x**2)*cos(x)
		>>> sp.diff(sp.sin(x**2), x)
		2*x*cos(x**2)
		\end{codeblock}

		% SYMPY DERIVATIVES
		%
		%
		%	\noindent
		%	In words,
		%	this calculation tells us the derivative of the function $f(x) = mx +b$ is the constant function $f^{\prime\!}(x)=m$.
		%	The expression \tt{diff(f,x)} tells SymPy to compute the derivative of the expression \tt{f} with respect to the variable \tt{x}.
		%
		%	Let's now define the function $f(x) = \frac{c}{2}x^2$ and compute its derivative.
		%
		%	\begin{codeblock}[sympy-diff-quadratic]
		%	>>> f = c/2 * x**2
		%	>>> diff(f, x)
		%	c*x
		%	\end{codeblock}
		%
		%	\noindent
		%	The derivative function is $f^{\prime\!}(x)=cx$.
		%	See the plot in Figure~\ref{fig:derivative_as_slope_xsq} for an illustration of the case when $c=1$.
		%
		%	Here is another example of a complicated-looking function $f$,
		%	that includes an exponential, a trigonometric, and a logarithmic function:
		%
		%	\begin{codeblock}[sympy-diff-fancy-mix]
		%	>>> from sympy import log, exp, sin
		%	>>> f = exp(x) + sin(x) + log(x)
		%	>>> diff(f, x)
		%	exp(x) + cos(x) + 1/x
		%	\end{codeblock}
		%
		%	\noindent
		%	As you can see,
		%	using the function SymPy function \tt{diff} allows you to compute the derivative function for any function $f(x)$.


    
	\subsection{Applications of derivatives}

		We use derivatives to solve problems in physics, chemistry, computing, biology, business,
		and many other areas of science.
		The derivative operator comes up whenever we study the rate of change of a quantity.
		% We use derivatives to obtain local liner approximations to functions (tangent lines).

		\subsubsection{Tangent lines}

			The \emph{tangent line} to the function $f(x)$ at $x=x_0$ is
			the line that passes through the point $(x_0, f(x_0))$ and has 
			the same slope as the function at that point.
			The tangent line to the function $f(x)$ at the point $x=x_0$ is described by the equation
			\[
			   T_1(x) =  f(x_0) \; + \;  f^{\prime\!}(x_0)(x-x_0).
			\]
			For example,
			the tangent line to $f(x)=x^2$ at $x_0=1$
			is $T_1(x) = f(1)  +  f^{\prime\!}(1)(x-1) = 1 + 2(x-1) = 2x - 1$.
			Look at the right side of Figure~\ref{fig:derivative_as_slope_xsq}
			for an illustration of this tangent line.
			
			The tangent line $T_1$ is also called a \emph{first order approximation} to the function $f$,
			since it has the same value and the same derivative as the function $f$,
			$T_1(1)= f(1)$ and $T'_1(1) = f^{\prime\!}(1)$.
			% The tangent line $T_1(x)$ has the same value and slope as the function $f(x)$ at $x=1$.
			In Section~\ref{series:taylor_series},
			we'll learn how to build a fancier approximation $T_n(x)$
			that matches the second, third, and higher derivatives of $f(x)$.


	\subsection{Optimization}

		Derivatives are used to solve optimization problems,
		which consist of finding the maximum or minimum value of some function $f(x)$.
		% One of the most prominent applications of derivatives is \emph{optimization}:
		% the process of finding a function's \emph{maximum} and \emph{minimum} values.
		For example,
		look the graph of the function $f(x)=x^2$
		in Figure~\ref{fig:derivative_as_slope_xsq}.
		The minimum of the function occurs when $x=0$.
		This is the bottom of a valley.

Observations:
1. The slope of the function is zero $f^{\prime\!}(0)=0$.
2. The second derivative of is positive at that point $f^{\prime\prime\!}(0) > 0$,
	which tells us the function looks locally like bottom of a valley.
We'll use these observations to come up with a mathematical (analytical)
optimization algorithm:
a strategy for finding the minimum(s) and the maximum(s) of a function.


		\subsubsection{Analytical optimization}

			The values of $x$ where the derivative is zero
			are called the \emph{critical points} of the function and denoted $x_1^*$, $x_2^*$, etc.
			Optimum values (maximum or minimum)
			occurs at a critical point of the function.
			We identify a critical point $x_j^*$,
			that corresponds to minimum
			if the second derivative is positive at that point $f^{\prime\prime\!}(x_j^*)>0$ (positive curvature).
			In contrast,
			a critical point $x_k^*$ here $f^{\prime\prime\!}(x_k^*)< 0$ (negative curvature) is a maximum.			
			These observations lead us to the following procedure
			for finding minima and maxima of the function $f(x)$:

			\begin{enumerate}
				\item[(1)]	Solve $f^{\prime\!}(x)=0$ to find the critical points $[x_1^*, x_2^*, x_3^*, \ldots]$.
				\item[(2)]	For each critical point $x_i^*$,
						check to see if it is a maximum or a minimum
						by evaluating $f^{\prime\prime\!}(x_i^*)$:
						% \emph{second derivative test}
						\begin{itemize}
							\item	If $f^{\prime\prime}(x_i^*) < 0$ then $x_i^*$ is a max (mountain top)
							\item	If $f^{\prime\prime}(x_i^*) > 0$ then $x_i^*$ is a min (bottom of a valley).
							%	\item	If $f^{\prime\prime}(x^*) = 0$
							%		then the second derivative test fails.
							%		We must revert back to checking nearby values
							%		$f^{\prime\!}(x^*-\delta)$ and $f^{\prime\!}(x^*+\delta)$
							%		to determine if $x^*$ is a max, a min, or a saddle point.
						\end{itemize}
			\end{enumerate}

			\noindent
			We can also perform the check in step (2) visually
			by looking at the graph of the function,
			or by evaluating the slope of the function on the left and the right of the critical point.
			If $f^{\prime\!}(x^*-0.01)$ is negative and $f^{\prime\!}(x^*+0.01)$ is positive,
			the point $x^*$ is a minimum (like near $x*=0$ in Figure~\ref{fig:derivative_as_slope_xsq}).
			If $f^{\prime\!}(x^*-0.01)$ is positive and $f^{\prime\!}(x^*+0.01)$ is negative,
			then the point $x^*$ is a maximum.
			If $f^{\prime\!}(x^*-0.01)$ and $f^{\prime\!}(x^*+0.01)$ have the same sign,
			the value $x^*$ is a  \emph{saddle point},
			which is neither a minimum or a maximum.

			\paragraph{Example}
				
				Let's use the analytical optimization procedure
				to find the minimum and the maximum of the function $f(x)=x^3-2x^2+x$.
				First we calculate its derivative $f^{\prime\!}(x) = 3x^2 - 4x + 1 = 3(x - 1)(x - \frac{1}{3})$.
				Next we find the critical points
				by solving the equation $f^{\prime\!}(x)=0$,
				which gives us two critical points $x^*_1 = \frac{1}{3}$ and $x^*_2 = 1$.
				The second derivative of the function is $f^{\prime\prime\!}(x) = 6x -4$.
				For the critical value $x^*_1 = \frac{1}{3}$,
				we find $f^{\prime\prime\!}(\frac{1}{3}) = -2 < 0$,
				% $f(x)$ looks like the peak of a mountain near $x=\frac{1}{3}$.
				which tells us $x^*_1 = \frac{1}{3}$ is a maximum.
				For $x^*_2 = 1$,
				we find $f^{\prime\prime\!}(1) = 2$,
				% looks like the bottom of a valley at $x=1$.
				which tells us $x^*_2=1$ is a minimum.



		\subsubsection{Numerical optimization}

			Consider the shape of the function near a minimum value.
			The function is decreasing on the left of the minimum,
			and the increasing on the right of the minimum.
			This means we can start at any point $x=x_0$ % near the minimum
			and take ``downhill'' steps to get to the minimum value.
			This simple procedure that repeatedly takes steps in the direction where the function is decreasing
			is a powerful algorithm that can find the minimum of any function.
			This procedure is called the \emph{gradient descent algorithm},
			where the name \emph{gradient} % (denoted $\nabla f(x,y)$)
			refers to the derivative operation for multivariable functions. % like $f(x,y)$.
			% TODO: say see notebook for implementation of derivative_descent algo. /TODO
			%	You'll encounter the gradient descent algorithm and its numerous variations
			%	if you choose to purse the topic of machine learning,
			%	since they are used in many machine learning applications.	% where multivariable functions are the norm
	
	
			\begin{codeblock}[def-derivative_descent]
			>>> def gradient_descent(f,x0=0,alpha=0.05,tol=1e-10):
			        current_x = x0
			        change = 1
			        while change > tol:
			            df_at_x = differentiate(f, current_x)
			            next_x = current_x - alpha * df_at_x
			            change = abs(next_x - current_x)
			            current_x = next_x
			        return current_x
			\end{codeblock} 


			\noindent
			The \tt{gradient\_descent} procedure takes two arguments as inputs:
			the function to minimize,
			and a initial value $x_0$ where to start the minimization process.

			Let's use this procedure to find the minimum of the function $q(x)=(x-5)^2$
			using the initial initial value $\tt{x0} = 10$
			as the starting point of the gradient descent algorithm.
	
			\begin{codeblock}[use-gradient_descent-on-f]
			>>> def q(x):
			        return (x - 5)**2
			>>> gradient_descent(q, x0=0)
			5.000000000396651
			\end{codeblock}
			
			\noindent
			The \tt{while} loop inside the \tt{gradient\_descent} function ran several times,
			and in each iteration takes a small ``downhill'' step
			until we get close to the minimum (the bottom of the valley).
			The procedure reaches the point $x=5.000000000396651$,
			which is close to true minimum of the function $q(x)$.
			% $\textrm{argmin}_x q(x) = 5$.

		
		\subsubsection{Numerical optimization using SciPy}
	
			%	In this book,
			%	we won't discuss the details behind optimization algorithms,
			%	and instead rely on the computational tools available in \tt{numpy}, \tt{scipy}, and \tt{sympy} to do optimization-type calculations for us.
			%	We'll encounter optimization ideas (maximization and minimization) in several concepts in statistics:
			%	\emph{maximum likelihood} and \emph{least squares},
			%	and rely on ``visual proofs'' for these optimization procedures.
			%	If you're interested in attaining a deeper understanding of optimization algorithms,
			%	you can follow the links provided at the end of this section,
			%	but note such ``under the hood'' understanding is not required to continue with the rest of the book.
			
			The module \tt{scipy.optimize}
			provides a high-performance numerical optimization procedure called \tt{minimize}
			that runs much faster than the function \tt{gradient\_descent} that we defined above.
			Here is a quick example that shows how to use \tt{minimize}
			to find the minimum of the function $q(x)=(x-5)^2$.

			\begin{codeblock}[sympy-minimize-fx]
			>>> from scipy.optimize import minimize
			>>> res = minimize(q, x0=0)
			>>> res["x"][0]  # = argmin q(x)
			4.9999999737
			\end{codeblock}
	
			\noindent
			Once more,
			the value we obtain $4.9999999737$
			is very close to the true minimum of the function $x^*=5$.



	

%!TEX root = ../calculus_tutorial.tex

\section{Derivatives}
\label{sec:derivatives}

	The \emph{derivative} function, denoted $f^{\prime\!}(x)$, $\frac{d}{dx}f(x)$, or $\frac{df}{dx}$,
	describes the \emph{rate of change} of the function $f(x)$.
	For example,
	the constant function $f(x)=c$ has derivative $f^{\prime\!}(x)=0$ since it doesn't change.
	Geometrically,
	the derivative function describes the \emph{slope} of the graph of the function $f(x)$.
	The derivative of the line $f(x)=mx+b$ is $f^{\prime\!}(x)=m$,
	since the slope of this line is equal to $m$.
	For general curves,
	the slope of a function will change at different values of~$x$,
	so mathematicians invented the notation $f^{\prime\!}(x)$
	for describing ``the slope of the function $f$ at $x$.''

	% Before we look at the definition of the derivative,
	Let's start by calculate the \emph{average} slope of the function between two points.
	Consider the points $(x,f(x))$ and $(x+\Delta x, f(x+\Delta x))$ on the graph of the function.
	We'll denote the horizontal distance that separates the two points $\Delta x$ (read \emph{delta} x),
	and similarly denote the vertical distance between the points as $\Delta y = f(x+\Delta x) - f(x)$.
	We can obtain the average slope of the function using the rise-over-run formula:
	$m = \frac{\Delta y}{\Delta x} = \frac{f(x+\Delta x) \; - \;f(x)}{x+\Delta x \; - \; x}$.
	Figure~\ref{fig:slope_calculations} illustrates the result of the average slope calculations
	at $x=1$ for different horizontal distance $\Delta x$.
	When $\Delta x = 2$ the average slope is $m = \frac{\Delta y}{\Delta x} = \frac{8}{2} = 4$.
	When $\Delta x = 1$ we get $m = \frac{\Delta y}{\Delta x} = \frac{3}{1} = 3$.
	and where $\Delta x = 0.3$ the slope is $m = \frac{\Delta y}{\Delta x} = \frac{0.69}{0.3} = 2.3$.
	If we continue this process with even smaller $\Delta x$,
	we'll obtain the \emph{instantaneous} slope at the point $x$.

	\begin{figure}[htb]
		\centering
		\includegraphics[width=0.99\columnwidth]{figures/calculus/slope_calculations.pdf}
		\caption{	Calculating the slope of the function $f(x) = x^2$
				by finding the line that passes through the points $(x,f(x))$
				and $(x+\Delta x, f(x+\Delta x))$.}
		\label{fig:slope_calculations}
	\end{figure}


	The derivative function $f^{\prime\!}(x)$ is defined as the following limit:
	\[
		f^{\prime\!}(x)
			\eqdef \lim_{\delta \to 0} \frac{f(x+\delta)\ - \ f(x)}{\delta}\,.
	\]
	In words,
	this formula describes the rise-over-run calculation
	for an infinitely short horizontal distance $\delta$.
	
	The derivative is a function of the form $f^{\prime\!}: \mathbb{R} \to \mathbb{R}$.
	It takes the value $x$ as input and tells you the slope the function $f$ at that value.
	Figure~\ref{fig:derivative_as_slope_xsq} shows the slope of the function $f(x) = x^2$
	at two different locations: at $x=-0.5$ and at $x=1$.
	% the slope of the function is the same as the line passing through this point

	\begin{figure}[htb]
		\centering
		\includegraphics[width=0.99\columnwidth]{figures/calculus/derivative_as_slope_xsq.pdf}
		\caption{	The derivative of the function at $x=a$
				is denoted $f^{\prime\!}(a)$
				and describes the slope function at that point.
				You can visually confirm the slope calculation using the mini grids drawn near each point.}
		\label{fig:derivative_as_slope_xsq}
	\end{figure}

	The derivative function $f^{\prime\!}(x)$ is a property of the function $f(x)$.
	Indeed, this is where the name \emph{derivative} comes from:
	$f^{\prime\!}(x)$ is not an independent function---it
	is \emph{derived} from the slope property of the original function $f(x)$.
	More generally,
	we can define the \emph{derivative operation},
	denoted $\frac{d}{dx}[\tt{<f>}]$,		% ALT. derivative operator $\frac{d}{dx}$
	which takes as input a function $f(x)$ and produces as output the derivative function $f^{\prime\!}(x)$.
	% which is a function of the form $f': \mathbb{R} \to \mathbb{R}$.
	% For each input $x_0$ the derivative function tells you the slope of $f(x)$ when $x=x_0$.
	Applying the derivative operation to the function
	is also called ``taking the derivative'' of a function.
	For example, 
	the derivative of the function $f(x) = x^2$ is the function $f^{\prime\!}(x) = 2x$.
	We can also describe this relationship as $(x^2)^{\prime} = 2x$
	or as $\tfrac{d}{dx}(x^2) = 2x$.
	Look at the graph in Figure~\ref{fig:derivative_as_slope_xsq} to convince yourself
	that the slope of $f(x)=x^2$ is indeed described by $f^{\prime\!}(x)=2x$ for any $x$.
	For example,
	when $x=0$,
	we see the graph has zero slope
	and the derivative gives us the same thing: $f^{\prime\!}(0) = 0$.


	\subsection{Numerical derivative calculations}

		Here is the Python code for computing
		a numerical approximations to the derivative the function \tt{f} at the point~\tt{x}:

		\begin{codeblock}[]
		>>> def differentiate(f, x, delta=1e-9):
		        df = f(x+delta) - f(x)
		        dx = delta
		        return df / dx
		\end{codeblock}

		\noindent
		The function \tt{differentiate} calculates the derivative using a finite step $\tt{delta} = 10^{-9}$
		instead of the infinitely small step $\delta$ in the math definition of the derivative.
		This means,
		the value returned by \tt{differentiate} will be an approximation to the true derivative.

		Let's now define a Python function \tt{f} that corresponds to the math function $f(x)=x^2$
		and use \tt{differentiate} to find the slope of $f$ when $x=1$:

		\begin{codeblock}[]
		>>> def f(x):
		        return x**2
		>>> differentiate(f, 1)
		2.000000165480742
		\end{codeblock}

		\noindent
		Using the numerical method,
		we obtain the approximation $f^{\prime\!}(1) = 2.000000165480742$,
		which is not perfect,
		but pretty close to the true value $f^{\prime\!}(1) = 2$.
		For most practical applications,
		this numerical approximation is good enough.

		%	The mathematical definition
		%	$f^{\prime\!}(x) \eqdef \lim_{ \delta \rightarrow 0}\frac{f(x+\delta)-f(x)}{\delta}$
		%	allows us to find a closed form expression for the derivative function
		%	that applies for all values of $x$.
		%	Finding the exact formula for the derivative requires a little bit more work upfront
		%	(to simplify an expressions that involves Greek symbols),
		%	but once you find the exact formula for the derivative $f^{\prime\!}(x)$,
		%	you can compute the slope of $f(x)$ at the point $x=c$
		%	by simply evaluating the the derivative function at that point:
		%	$f^{\prime\!}(c) = \{ \textrm{the slope of} \ f(x) \ \textrm{at} \ x=c\}$.



	\subsection{Derivative formulas}

		You don't need to apply the complicated derivative formula
		$f^{\prime\!}(x) \eqdef \lim_{\delta \to 0}\frac{f(x+\delta)-f(x)}{\delta}$
		every time you need to find the derivative of a function.
		For each function $f(x)$,
		it's enough to use the complicated formula once
		and record the formula you obtain for $f^{\prime\!}(x)$,
		then you can reuse that formula
		whenever you need to compute $f^{\prime\!}(x)$ in later calculations.

		Table~\ref{table:derivatives} shows the derivatives several functions.
		I invite you to mentally bookmark this page so you can come back to it
		when you need to know the derivatives of some function.

		\begin{table}[htb]
			\centering
			\caption{Derivative formulas for commonly used functions}
			\begin{shadebox}%
			\vspace{-2mm}
			\begin{align*}
			f(x)					&  \ -\textrm{derivative}\to  \   				f^{\prime\!}(x)	\\
			a					&\qquad  - \tfrac{d}{dx} \rightarrow \qquad 		0			\\
			x					&\qquad  - \tfrac{d}{dx} \rightarrow \qquad		1			\\
			mx+b				&\qquad  - \tfrac{d}{dx} \rightarrow \qquad		m			\\
			x^n, \text{ for } n\neq 0	&\qquad  - \tfrac{d}{dx} \rightarrow \qquad 		nx^{n-1}		\\
			\tfrac{1}{x} =  x^{-1}		&\qquad  - \tfrac{d}{dx} \rightarrow \qquad 		\tfrac{-1}{x^2} = -x^{-2}	\\
			\sqrt{x} = x^{\frac{1}{2}}	&\qquad  - \tfrac{d}{dx} \rightarrow \qquad		\tfrac{1}{2\sqrt{x}} = \tfrac{1}{2}x^{-\frac{1}{2}} \\
			e^x					&\qquad  - \tfrac{d}{dx} \rightarrow \qquad 		e^x			\\
			\ln(x)					&\qquad  - \tfrac{d}{dx} \rightarrow \qquad 		\tfrac{1}{x}		\\
			\sin(x)				&\qquad  - \tfrac{d}{dx} \rightarrow \qquad 		\cos(x)		\\
			\cos(x)				&\qquad  - \tfrac{d}{dx} \rightarrow \qquad 		-\sin(x)
			\end{align*}%
			\end{shadebox}
			\label{table:derivatives}
		\end{table}
		%	\alpha f(x)+ \beta g(x)	&\qquad  - \tfrac{d}{dx} \rightarrow \qquad 		\alpha f^{\prime\!}(x)+ \beta g^{\prime\!}(x)	\\
		%	af(x)			&\qquad  - \tfrac{d}{dx} \rightarrow \qquad 				af^{\prime\!}(x)		\\
		%	f(x)+g(x)		&\qquad  - \tfrac{d}{dx} \rightarrow \qquad 				f^{\prime\!}(x)+g^{\prime\!}(x)	\\
		% 	a^x			&\qquad  - \tfrac{d}{dx} \rightarrow \qquad 				a^x\ln(a)	\\
		% 	\log_a(x)		&\qquad  - \tfrac{d}{dx} \rightarrow \qquad 				(x\ln(a))^{-1}	\\
		% 	\tan(x)		&\qquad  - \tfrac{d}{dx} \rightarrow \qquad 				\sec^2(x) = \cos^{-2}(x)
		%	\csc(x) = \frac{1}{\sin(x)}	&\qquad  - \tfrac{d}{dx} \rightarrow \qquad 		-\sin^{-2}(x)\cos(x)	\\
		%	\sec(x) = \frac{1}{\cos(x)}	&\qquad  - \tfrac{d}{dx} \rightarrow \qquad 		\tan(x)\sec(x)	\\
		%	\cot(x) = \frac{1}{\tan(x)}	&\qquad  - \tfrac{d}{dx} \rightarrow \qquad 		-\csc^2(x)	\\
		%	\sin^{-1}(x)	&\qquad  - \tfrac{d}{dx} \rightarrow \qquad 				\frac{1}{\sqrt{1-x^2}}	\\
		%	\cos^{-1}(x)	&\qquad  - \tfrac{d}{dx} \rightarrow \qquad 				\frac{-1}{\sqrt{1-x^2}}	\\
		%	\tan^{-1}(x)	&\qquad  - \tfrac{d}{dx} \rightarrow \qquad 				\frac{1}{1+x^2}	\\
		%	\sinh(x)		&\qquad  - \tfrac{d}{dx} \rightarrow \qquad 				\cosh(x)	\\
		%	\cosh(x)		&\qquad  - \tfrac{d}{dx} \rightarrow \qquad 				\sinh(x)	
		\noindent
		Table~\ref{table:derivatives} presents the results
		in terms of the derivative operator $\frac{d}{dx}[\tt{<f>}]$,
		which takes as input some function $f(x)$
		and produces as output its derivative function $f^{\prime\!}(x)$.


	\subsection{Derivative rules}

		In addition to the table of derivative formulas,
		there are some important derivatives rules
		that allow you to find derivatives of \emph{composite} functions.

		\paragraph{Constant multiple rule}

			The derivative of $k$ times the function $f(x)$
			is equal to $k$ times the derivative of $f(x)$:
			\[
				\big[ k f(x) \big]^\prime		= 	k f^{\prime\!}(x).
			\]

		\paragraph{Sum rule}

			The derivative of the sum of two functions
			is the sum of their derivatives:
			\[
				\big[ f(x) +  g(x) \big]^\prime	=	f^{\prime\!}(x) + g^{\prime\!}(x).
			\]

			%	The derivative of a \emph{linear combination} of functions $\alpha f(x) + \beta g(x)$ is equal 
			%	to the same linear combination of the derivatives $\alpha f^{\prime\!}(x) + \beta g^{\prime\!}(x)$.


		\paragraph{Product rule}

			The derivative of a product of two functions
			is the sum of two contributions:
			\[
				\big[ f(x)g(x) \big]^\prime 		= 	f^{\prime\!}(x)g(x)  + f(x)g^{\prime\!}(x).
			\]
			In each term,
			the derivative of one of the functions
			is multiplied by the value of the other function.


		\paragraph{Quotient rule}

			This formula tells us how to obtain the derivative of a fraction of two functions:
			\[
				\left[ \frac{f(x)}{g(x)} \right]^\prime =	\frac{f^{\prime\!}(x)g(x)-f(x)g^{\prime\!}(x)}{g(x)^2}.
			\]


		\paragraph{Chain rule}
			If you encounter a situation that includes an inner function and an outer function,
			like $f(g(x))$, you can obtain the derivative by a two-step process:
			\[
				\big[ f(g(x)) \big]^\prime	= 	f^{\prime\!}(g(x))g^{\prime\!}(x).
			\]

			\noindent
			In the first step,
			we leave the inner function $g(x)$ alone
			and focus on taking the derivative of the outer function $f(x)$.
			This step gives us $f^{\prime\!}(g(x))$,
			which is the value of $f^{\prime}$ evaluated at $g(x)$.
			In the second step,
			we multiply this expression by the derivative of the \emph{inner} function $g^{\prime\!}(x)$.



	\subsection{Higher derivatives}

		The second derivative of $f(x)$ is denoted $f^{\prime\prime\!}(x)$ or $\frac{d^2f}{dx^2}$.
		It is obtained by applying the derivative operation to $f(x)$ \emph{twice}:
		$\frac{d}{dx}\big[ \frac{d}{dx}[\tt{<f>}] \big]$.
		Geometrically,
		the second derivative $f^{\prime\prime\!}(x)$
		tells us the \emph{curvature} of $f(x)$.
		Positive curvature means the function opens upward
		and looks like the bottom of a valley.
		The function $f(x)=x^2$ shown in Figure~\ref{fig:derivative_as_slope_xsq} 
		has derivative $f^{\prime\!}(x) = 2x$ and second derivative $f^{\prime\prime\!}(x) = 2$,
		which means it has positive curvature.
		Negative curvature means the function opens downward
		and looks like a mountain peak.
		For example,
		the function $g(x) = -x^2$ has negative curvature.



	\subsection{Examples}

		Armed with the derivative formulas from Table~\ref{table:derivatives} and the derivative rules from the previous section,
		you can the derivative of any function,
		no matter how complicated.
		Let's look at some examples.

		\paragraph{Example 1}
			To calculate the derivative of $f(x) = e^{x^2}$,
			we use the chain rule: $f^{\prime\!}(x) = e^{x^2}[x^2]'  = e^{x^2}2x$.

		\paragraph{Example 2}
			To find the derivative of $f(x) = \sin(x)e^{x^2}$,
			we use the product rule and the chain rule: $f^{\prime\!}(x) = \cos(x)e^{x^2} + \sin(x)2xe^{x^2}$.

		\paragraph{Example 3}
			The derivative of $\sin(x^2)$ requires using the chain rule:
			$\left[ \sin(x^2) \right]^\prime =  \cos(x^2)\left[x^2\right]' =  \cos(x^2)2x$.




	\subsection{Computing derivatives using SymPy}

		The \texttt{SymPy} function \texttt{sp.diff} computes the derivative of any expression.
		For example,
		here is how to compute the derivative of the function $f(x) = mx +b$:

		\begin{codeblock}[sympy-diff-line]
		>>> m, x, b = sp.symbols("m x b")
		>>> sp.diff(m*x + b, x)
		m
		\end{codeblock}

		\noindent		
		Let's also verify the derivative formula $\tfrac{d}{dx}[x^n] = nx^{n-1}$:

		\begin{codeblock}[]
		>>> x, n = sp.symbols("x n")
		>>> sp.diff(x**n, x)
		n * x**(n - 1)
		\end{codeblock}

		\noindent
		The exponential function $f(x)=e^x$ is special
		because it is the only function that is equal to its derivative:

		\begin{codeblock}[]
		>>> from sympy import exp
		>>> sp.diff(exp(x), x)
		exp(x)
		\end{codeblock}

		\noindent
		
		Here is an example of the derivative
		of function that includes exponential, trigonometric, and logarithmic terms:
	
		\begin{codeblock}[sympy-diff-fancy-mix]
		>>> from sympy import exp, sin, log
		>>> sp.diff(exp(x) + sin(x) + log(x), x)
		exp(x) + cos(x) + 1/x
		\end{codeblock}

		\noindent
		Let's check the derivative calculations from the examples:

		\begin{codeblock}[]
		>>> sp.diff(sp.exp(x**2), x)
		2*x*exp(x**2)
		>>> sp.diff(sp.sin(x)*sp.exp(x**2), x)
		2*x*exp(x**2)*sin(x) + exp(x**2)*cos(x)
		>>> sp.diff(sp.sin(x**2), x)
		2*x*cos(x**2)
		\end{codeblock}

		As you can see,
		the function \tt{sp.diff} gives the same answers.



    
	\subsection{Applications of derivatives}

		Derivatives are used in physics, chemistry, computing, biology, business,
		and many other areas of science.
		We need derivatives whenever we compute rates of change of quantities.


		\subsubsection{Tangent lines}

			% We use derivatives to obtain local liner approximations to functions (tangent lines).
			The \emph{tangent line} to the function $f(x)$ at $x=x_0$ is
			the line that passes through the point $(x_0, f(x_0))$ and has 
			the same slope as the function at that point.
			The tangent line to the function $f(x)$ at the point $x=x_0$ is described by the equation
			\[
				T_1(x) =  f(x_0) \; + \;  f^{\prime\!}(x_0)(x-x_0).
			\]
			For example,
			the tangent line to $f(x)=x^2$ at $x_0=1$
			is $T_1(x) = f(1)  +  f^{\prime\!}(1)(x-1) = 1 + 2(x-1) = 2x - 1$.
			Look at the right side of Figure~\ref{fig:derivative_as_slope_xsq}
			for an illustration of this tangent line.

			The tangent line $T_1$ is also called a \emph{first order approximation} to the function $f$,
			since it has the same value and the same derivative as the function $f$,
			$T_1(1) = f(1)$ and $T'_1(1) = f^{\prime\!}(1)$.
			% The tangent line $T_1(x)$ has the same value and slope as the function $f(x)$ at $x=1$.
			In Section~\ref{series:taylor_series},
			we'll learn how to build a fancier approximation $T_n(x)$
			that matches the second, third, and higher derivatives of $f(x)$.

		Another application of derivatives is \emph{optimization}
		which deserves its own section!


	\subsection{Solving optimization problems using derivatives}
		
		We're often interesting in finding the values $x$ where some function $f(x)$
		reaches its \emph{minimum} value.
		Knowing the derivatives of the function $f(x)$
		is very useful for solving optimization problems.
		For example,
		let's look the graph of the function $f(x)=x^2$
		shown in Figure~\ref{fig:derivative_as_slope_xsq}.
		The minimum of this function occurs when $x=0$.
		We can make the following observations about the graph of the function at the minimum value:

		\begin{enumerate}

			\item[(A)] 	The slope of the function is negative on the left of the minimum,
					and positive on the right of the minimum.
					% ALT
					%	The function is decreasing on the left of the minimum,
					%	and the increasing on the right of the minimum.

			\item[(B)]	The slope of $f(x)$ is zero at the minimum: $f^{\prime\!}(0) = 0$.

			\item[(C)]	The graph of the function looks locally like bottom of a valley at $x=0$,
					This means the second derivative of $f(x)$
					is positive at that point $f^{\prime\prime\!}(0) > 0$.
			
		\end{enumerate}

		\noindent
		We can use these observations to come up
		with general strategies for finding the minimum of any function.
		We'll describe two different strategies below:
		the first one based on math formulas,
		the second one based on numerical computations.

		\subsubsection{Analytical optimization}

			The values of $x$ where the derivative is zero
			are called the \emph{critical points} of the function and denoted $x_1^*$, $x_2^*$, etc.
			Observation~(B) tells us that optimum values (maximum or minimum)
			occurs at a critical point of the function.
			Observation~(C) tells us that we can identify a critical point $x_j^*$,
			that corresponds to minimum
			if the second derivative is positive at that point $f^{\prime\prime\!}(x_j^*)>0$ (positive curvature).
			In contrast,
			a critical point $x_k^*$ here $f^{\prime\prime\!}(x_k^*)< 0$ (negative curvature) is a maximum.			
			Using these observations lead us to the following analytical procedure
			for finding minima and maxima of the function $f(x)$:

			\begin{enumerate}
				\item[(1)]	Solve $f^{\prime\!}(x)=0$ to find the critical points $[x_1^*, x_2^*, x_3^*, \ldots]$.
				\item[(2)]	For each critical point $x_i^*$,
						check to see if it is a maximum or a minimum
						by evaluating $f^{\prime\prime\!}(x_i^*)$:	% A.K.A. \emph{second derivative test}
						\begin{itemize}
							\item	If $f^{\prime\prime}(x_i^*) < 0$ then $x_i^*$ is a max (mountain top)
							\item	If $f^{\prime\prime}(x_i^*) > 0$ then $x_i^*$ is a min (bottom of a valley).
							%	\item	If $f^{\prime\prime}(x^*) = 0$
							%		then the second derivative test fails.
							%		We must revert back to checking nearby values
							%		$f^{\prime\!}(x^*-\delta)$ and $f^{\prime\!}(x^*+\delta)$
							%		to determine if $x^*$ is a max, a min, or a saddle point.
						\end{itemize}
			\end{enumerate}

			\noindent
			We can also perform the check in step (2) visually
			by looking at the graph of the function,
			or by evaluating the slope of the function on the left and the right of the critical point.
			If $f^{\prime\!}(x^*-0.01)$ is negative and $f^{\prime\!}(x^*+0.01)$ is positive,
			the point $x^*$ is a minimum (like near $x*=0$ in Figure~\ref{fig:derivative_as_slope_xsq}).
			If $f^{\prime\!}(x^*-0.01)$ is positive and $f^{\prime\!}(x^*+0.01)$ is negative,
			then the point $x^*$ is a maximum.
			If $f^{\prime\!}(x^*-0.01)$ and $f^{\prime\!}(x^*+0.01)$ have the same sign,
			the value $x^*$ is a  \emph{saddle point},
			which is neither a minimum or a maximum.

			\paragraph{Example 1}

				Let's apply the analytical optimization procedure
				to find the minimum value of the function $q(x)=(x-5)^2$.
				The derivative of the function is $q^{\prime\!}(x) = 2(x-5)$.
				Next,
				we find the critical point(s) by solving the equation $q^{\prime\!}(x)=0$,
				which has a single solution $x^*_1 = 5$.
				Is the critical value $x^*_1 = 5$ a minimum or a maximum?
				To find out,
				we compute the second derivative $q^{\prime\prime\!}(x) = 2$,
				and check its sign at the critical value: $q^{\prime\prime\!}(5) = 2 > 0$.
				The second derivative is positive (bottom of a valley),
				so this means $x^*_1 = 5$ is a minimum.


			\paragraph{Example 2}

				What are the minimum and maximum values of the function $r(x) = x^3-2x^2+x$.
				The derivative function is $r^{\prime\!}(x) = 3x^2 - 4x + 1 = 3(x - 1)(x - \frac{1}{3})$.
				We find the critical points by solving the equation $r^{\prime\!}(x)=0$,
				which leads us to two critical points $x^*_1 = \frac{1}{3}$ and $x^*_2 = 1$.
				The second derivative of the function is $r^{\prime\prime\!}(x) = 6x -4$.
				For the critical value $x^*_1 = \frac{1}{3}$,
				we find $r^{\prime\prime\!}(\frac{1}{3}) = -2 < 0$,
				which tells us $x^*_1 = \frac{1}{3}$ is a maximum.
				For $x^*_2 = 1$,
				we find $r^{\prime\prime\!}(1) = 2$,
				so $x^*_2=1$ is a minimum.



		\subsubsection{Numerical optimization}

			Observation~(A) suggests another way to find the minimum of a function:
			if we repeatedly take steps in the ``downhill'' direction,
			we'll end up at the bottom of a valley.
			This is the idea behind the \emph{gradient descent algorithm},
			which allows us to find find the minimum of any function.
			We start at some point $x=x_0$
			and repeatedly take steps in the direction where the function is decreasing.
			% The name \emph{gradient} refers to the derivative operation for multivariable functions.
			% The more appropriate name in this context would be \emph{derivative descent algorithm},
			% since we're using the information from the derivative of the function to find bottom of the valley.

			\begin{codeblock}[def-gradient_descent]
			>>> def gradient_descent(f,x0=0,alpha=0.05,tol=1e-10):
			        current_x = x0
			        change = 1
			        while change > tol:
			            df_at_x = differentiate(f, current_x)
			            next_x = current_x - alpha * df_at_x
			            change = abs(next_x - current_x)
			            current_x = next_x
			        return current_x
			\end{codeblock} 

			The  \tt{gradient\_descent} procedure takes two arguments as inputs:
			the function we want to minimize \tt{f},
			and a initial value $x_0$ where to start the minimization process.
			The procedure then visits the points $x_1$, $x_2$, $x_3$, etc.,
			by repeatedly taking steps
			in the direction opposing the derivative at the current $x$.
			The formula $x_{i+1} = x_i - \alpha f'(x_i)$ is used to find the next point,
			where the step size is determined by the parameter $\alpha$
			and the slope of the function.

			Here is how to use \tt{gradient\_descent}
			to find the minimum of the functions $q(x) = (x-5)^2$ and $r(x) = x^3-2x^2+x$,
			using the value $\tt{x0} = 10$ as the starting point of the gradient descent.

			\begin{codeblock}[use-gradient_descent-on-q-and-r]
			>>> def q(x):
			        return (x - 5)**2
			>>> gradient_descent(q, x0=10)
			5.000000000396651
			>>> def r(x):
			        return x**3 - 2*x**2 + x
			>>> gradient_descent(r, x0=10)
			1.0000000932587236
			\end{codeblock}

			\noindent
			The \tt{while} loop in the \tt{gradient\_descent} procedure ran many times,
			and in each iteration took a small downhill step
			until we got to the minimum (the bottom of the valley).
			The optimization procedure returned the values $x=5.000000000396651$
			and $x=1.0000000932587236$,
			which are close to true minimum values of the functions $q(x)$ and $r(x)$.
			% $\textrm{argmin}_x q(x) = 5$.




		\subsubsection{Numerical optimization using SciPy}

			The Python module SciPy % \tt{scipy.optimize}
			provides a high-performance numerical optimization procedure called \tt{minimize}
			that runs much faster than the \tt{gradient\_descent} procedure that we defined above.
			Here is a demonstration that shows how we use the function \tt{minimize}
			to find the minima of the functions $q(x)$ and $r(x)$. % from the previous examples.

			\begin{codeblock}[sympy-minimize-fx]
			>>> from scipy.optimize import minimize
			>>> minimize(q, x0=10)["x"][0]
			4.9999999737
			>>> minimize(r, x0=10)["x"][0]
			1.0000004142283734
			\end{codeblock}

			\noindent
			Once more,
			we obtain approximate values
			that are very close to the true minimum values of the functions $q(x)$ and $r(x)$.





%	We'll encounter optimization ideas (maximization and minimization) in several concepts in statistics:
%	\emph{maximum likelihood} and \emph{least squares},
%	and rely on ``visual proofs'' for these optimization procedures.
%	If you're interested in attaining a deeper understanding of optimization algorithms,
%	you can follow the links provided at the end of this section,
%	but note such ``under the hood'' understanding is not required to continue with the rest of the book.

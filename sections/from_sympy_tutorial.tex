%!TEX root = ../calculus_tutorial.tex

\section{Calculus using SymPy}
% \label{}	


Calculus is the study of the properties of functions.
The operations of calculus are used to describe the limit behaviour of functions,
calculate their rates of change,
and calculate the areas under their graphs.
In this section we'll learn about the \texttt{SymPy} functions for calculating
limits, derivatives, integrals, and summations.

\subsection{Infinity}
\label{calculus:infinity}

\small
\begin{verbatimtab}
from sympy import oo
\end{verbatimtab}
\normalsize

\noindent
The infinity symbol is denoted \texttt{oo} (two lowercase \texttt{o}s) in \texttt{SymPy}.						\index{infinity}
Infinity is not a number but a process: the process of counting forever.
Thus, $\infty + 1 = \infty$, 
$\infty$ is greater than any finite number,
and $1/\infty$ is an infinitely small number.
Sympy knows how to correctly treat infinity in expressions:

\small
\begin{verbatimtab}
>>> oo+1
oo
>>> 5000 < oo 
True
>>> 1/oo
0
\end{verbatimtab}
\normalsize

\subsection{Limits}
\label{calculus:limits}
\small
\begin{verbatimtab}
from sympy import limit
\end{verbatimtab}
\normalsize

\noindent
We use limits to describe, with mathematical precision, infinitely large quantities,						\index{limit}
infinitely small quantities, and procedures with infinitely many steps.

The number $e$ is defined as the limit
$\displaystyle e \eqdef \lim_{n\to \infty} \left( 1 + \frac{1}{n}\right)^{n}$:
\small
\begin{verbatimtab}
>>> limit( (1+1/n)**n, n, oo)
E          # = 2.71828182845905
\end{verbatimtab}
\normalsize

\noindent
This limit expression describes the annual growth rate of a loan
with a nominal interest rate of $100\%$ and infinitely frequent compounding.							\index{interest rate}
Borrow $\$1000$ in such a scheme,
and you'll owe $\$2718.28$ after one~year.

Limits are also useful to describe the behaviour of functions.
Consider the function $f(x)=\frac{1}{x}$.
The \texttt{limit} command shows us what happens to $f(x)$ near $x=0$ and as $x$ goes to infinity:

\small
\begin{verbatimtab}
>>> limit( 1/x, x, 0, dir="+")
oo
>>> limit( 1/x, x, 0, dir="-")
-oo
>>> limit( 1/x, x, oo)
0
\end{verbatimtab}
\normalsize

\noindent
As $x$ becomes larger and larger, the fraction $\frac{1}{x}$ becomes smaller and smaller.
In the limit where $x$ goes to infinity, $\frac{1}{x}$ approaches zero: $\lim_{x\to \infty} \frac{1}{x} = 0$. 
On the other hand, when $x$ takes on smaller and smaller positive values,
the expression $\frac{1}{x}$ becomes infinite: $\lim_{x \to 0^+} \frac{1}{x} = \infty$.
When $x$ approaches $0$ from the left, we have $\lim_{x \to 0^-} \frac{1}{x} = -\infty$.
If these calculations are not clear to you,
study the graph of $f(x)=\frac{1}{x}$.


Here are some other examples of limits:

\small
\begin{verbatimtab}
>>> limit(sin(x)/x, x, 0)
1
>>> limit(sin(x)**2/x, x, 0)
0
>>> limit(exp(x)/x**100,x,oo) # which is bigger e^x or x^100 ?
oo                            # exp f >> all poly f for big x  
\end{verbatimtab}
\normalsize

Limits are used to define the derivative and the integral operations.

\subsection{Derivatives}
\label{calculus:derivatives}
The derivative function, denoted $f'(x)$, $\frac{d}{dx}f(x)$, $\frac{df}{dx}$, or $\frac{dy}{dx}$, 								\index{derivative}
describes the \emph{rate of change} of the function $f(x)$.
The \texttt{SymPy} function \texttt{diff} computes the derivative of any expression:



\small
\begin{verbatimtab}
>>> diff(x**3, x)
3*x**2
\end{verbatimtab}
\normalsize

\noindent
The differentiation operation knows the product rule $[f(x)g(x)]^\prime=f^\prime(x)g(x)+f(x)g^\prime(x)$, 
the chain rule $f(g(x))' = f'(g(x))g'(x)$, 
and the quotient rule $\left[\frac{f(x)}{g(x)}\right]^\prime = \frac{f'(x)g(x) - f(x)g'(x)}{g(x)^2}$:



\small
\begin{verbatimtab}
>>> diff( x**2*sin(x), x )
2*x*sin(x) + x**2*cos(x)
>>> diff( sin(x**2), x )
cos(x**2)*2*x
>>> diff( x**2/sin(x), x )
(2*x*sin(x) - x**2*cos(x))/sin(x)**2
\end{verbatimtab}
\normalsize

\noindent
The second derivative of a function \texttt{f} is \texttt{diff(f,x,2)}:



\small
\begin{verbatimtab}
>>> diff(x**3, x, 2)       # same as diff(diff(x**3, x), x)
6*x
\end{verbatimtab}
\normalsize


\subsection{Tangent lines}
\label{calculus:tangent_lines}

The \emph{tangent line} to the function $f(x)$ at $x=x_0$ is 													\index{tangent line}
the line that passes through the point $(x_0, f(x_0))$ and has 
the same slope as the function at that point.
The tangent line to the function $f(x)$ at the point $x=x_0$ is described by the equation
\[
   T_1(x) =  f(x_0) \; + \;  f'(x_0)(x-x_0).
\]
What is the equation of the tangent line to $f(x)=\frac{1}{2}x^2$ at $x_0=1$?



\small
\begin{verbatimtab}
>>> f = S('1/2')*x**2
>>> f
x**2/2
>>> df = diff(f, x)
>>> df
x
>>> T_1 = f.subs({x:1}) + df.subs({x:1})*(x - 1)
>>> T_1
x - 1/2           #  y = x - 1/2
\end{verbatimtab}
\normalsize

\noindent
The tangent line $T_1(x)$ has the same value and slope as the function $f(x)$ at $x=1$:
\small
\begin{verbatimtab}
>>> T_1.subs({x:1}) == f.subs({x:1})
True
>>> diff(T_1, x).subs({x:1}) == diff(f, x).subs({x:1})
True
\end{verbatimtab}
\normalsize



\subsection{Optimization}
\label{calculus:optimization}
																								\index{optimization}


	Optimization is about choosing an input for a function $f(x)$ that results in the best value for $f(x)$.					
	The best value usually means the \emph{maximum} value 
	(if the function represents something desirable like profits) 
	or the \emph{minimum} value 
	(if the function represents something undesirable like costs).

	The derivative $f'(x)$ encodes the information about the \emph{slope} of $f(x)$.
	Positive slope $f'(x)>0$ means $f(x)$ is increasing,
	negative slope $f'(x)<0$ means $f(x)$ is decreasing, 
	and zero slope $f'(x)=0$ means the graph of the function is horizontal.
	The \emph{critical points} of a function $f(x)$ are the solutions to the equation $f'(x)=0$.
	Each critical point is a candidate to be either a maximum or a minimum of the function.

	The second derivative $f^{\prime\prime}(x)$ encodes the information about the \emph{curvature} of $f(x)$.
	Positive curvature means the function looks like~$x^2$,
	negative curvature means the function looks like $-x^2$.


%	Recall the \emph{second derivative test} for finding the maxima and minima of a function,
%	which we learned on page~\pageref{optimization_algorithm:alternate_algorithm}.

Let's find the critical points of the function $f(x)=x^3-2x^2+x$ and use the information from its second derivative 			\index{critical point}
to find the maximum of the function on the interval $x \in [0,1]$.													\index{maximum}

\small
\begin{verbatimtab}
>>> x = Symbol('x')
>>> f = x**3-2*x**2+x
>>> diff(f, x)
3*x**2 - 4*x + 1
>>> sols = solve( diff(f,x),  x)
>>> sols
[1/3, 1]
>>> diff(diff(f,x), x).subs( {x:sols[0]} )
-2
>>> diff(diff(f,x), x).subs( {x:sols[1]} )
2
\end{verbatimtab}
\normalsize

\noindent
\href{https://www.google.ca/\#q=plot+x**3-2*x**2++\%2B+x&safe=off}{It will help to look at the graph of this function.}
The point $x=\frac{1}{3}$ is a local maximum because it is a critical point of $f(x)$
where the curvature is negative, meaning $f(x)$ looks like the peak of a mountain at $x=\frac{1}{3}$.
The maximum value of $f(x)$ on the interval $x\in [0,1]$ is $f\!\left(\frac{1}{3}\right)=\frac{4}{27}$.
The point $x=1$ is a local minimum because it is a critical point													\index{minimum}
with positive curvature, meaning $f(x)$ looks like the bottom of a valley at $x=1$.




\subsection{Integrals}
\label{calculus:integrals}


The \emph{integral} of $f(x)$ corresponds to the computation of the area under the graph of $f(x)$.						\index{area}
The area under $f(x)$ between the points $x=a$ and $x=b$ is denoted as follows:
\[
 A(a,b) = \int_a^b f(x) \: dx.
\]
The \emph{integral function} $F$ corresponds to the area calculation as a function of the upper limit of integration:
\[
  F(c) \eqdef \int_0^c \! f(x)\:dx\,.
\]
The area under $f(x)$ between $x=a$ and $x=b$ is obtained by calculating the \emph{change} in the integral function:
\[
   A(a,b) = \int_a^b \! f(x)\:dx  =  F(b)-F(a).
\]


In \texttt{SymPy} we use \texttt{integrate(f, x)} to obtain the integral function $F(x)$ of any function $f(x)$:					\index{integral}
$F(x) = \int_0^x f(u)\,du$.

\small
\begin{verbatimtab}
>>> integrate(x**3, x)
x**4/4
>>> integrate(sin(x), x)
-cos(x)
>>> integrate(ln(x), x)
x*log(x) - x
\end{verbatimtab}
\normalsize
This is known as an \emph{indefinite integral} since the limits of integration are not defined. 

In contrast, 
a \emph{definite integral} computes the area under $f(x)$ between $x=a$ and $x=b$.
Use \texttt{integrate(f, (x,a,b))} to compute the definite integrals of the form $A(a,b)=\int_a^b f(x) \, dx$:

\small
\begin{verbatimtab}
>>> integrate(x**3, (x,0,1))    
1/4              # the area under x^3 from x=0 to x=1
\end{verbatimtab}
\normalsize

\noindent
We can obtain the same area by first calculating the indefinite integral $F(c)=\int_0^c \!f(x)\,dx$,
then using $A(a,b) = F(x)\big\vert_a^b = F(b) - F(a)$:



\small
\begin{verbatimtab}
>>> F = integrate(x**3, x)
>>> F.subs({x:1}) - F.subs({x:0})   
1/4
\end{verbatimtab}
\normalsize
Integrals correspond to \emph{signed} area calculations:



\small
\begin{verbatimtab}
>>> integrate(sin(x), (x,0,pi))
2
>>> integrate(sin(x), (x,pi,2*pi))
-2
>>> integrate(sin(x), (x,0,2*pi))
0
\end{verbatimtab}
\normalsize

\noindent
During the first half of its $2\pi$-cycle,
the graph of $\sin(x)$ is above the $x$-axis, so it has a positive contribution to the area under the curve.
During the second half of its cycle (from $x=\pi$ to $x=2\pi$),
$\sin(x)$ is below the $x$-axis, so it contributes negative area.
Draw a graph of $\sin(x)$ to see what is going on. 

\subsection{Fundamental theorem of calculus}
\label{calculus:fundamental_theorem_of_calculus}
																								\index{fundamental theorem of calculus}
The integral is the ``inverse operation'' of the derivative.			\index{inverse!operation}
If you perform the integral operation followed by the derivative operation on some function,
you'll obtain the same function:
\[
  \left(\frac{d}{dx} \circ \int dx \right) f(x) = \frac{d}{dx} \int_c^x f(u)\:du = f(x).
\]



\small
\begin{verbatimtab}
>>> f = x**2
>>> F = integrate(f, x)
>>> F
x**3/3           # + C
>>> diff(F, x)
x**2
\end{verbatimtab}
\normalsize

\noindent
Alternately, if you compute the derivative of a function followed by the integral,
you will obtain the original function $f(x)$ (up to a constant):
\[
  \left( \int dx \circ \frac{d}{dx}\right) f(x) = \int_c^x f'(u)\;du = f(x) + C.
\]



\small
\begin{verbatimtab}
>>> f = x**2
>>> df = diff(f, x)
>>> df
2*x
>>> integrate(df, x)
x**2    # + C
\end{verbatimtab}
\normalsize

\noindent
The fundamental theorem of calculus is important because it tells us how to solve differential equations.
If we have to solve for $f(x)$ in the differential equation $\frac{d}{dx}f(x) = g(x)$,
we can take the integral on both sides of the equation to obtain the answer $f(x) = \int g(x)\,dx + C$.

\subsection{Sequences}
\label{calculus:sequences}

Sequences are functions that take whole numbers as inputs.												\index{sequence}
Instead of continuous inputs $x\in \mathbb{R}$,
sequences take natural numbers $n\in\mathbb{N}$ as inputs.
We denote sequences as $a_n$ instead of the usual function notation $a(n)$.

We define a sequence by specifying an expression for its $n$\textsuperscript{th} term:



\small
\begin{verbatimtab}
>>> a_n = 1/n
>>> b_n = 1/factorial(n)
\end{verbatimtab}
\normalsize

\noindent
Substitute the desired value of $n$ to see the value of the $n$\textsuperscript{th} term:

\small
\begin{verbatimtab}
>>> a_n.subs({n:5})
1/5
\end{verbatimtab}
\normalsize

\noindent
%We can use 
The Python list comprehension syntax \texttt{[item for item in list]}
can be used to print the sequence values for some range of indices:



\small
\begin{verbatimtab}
>>> [ a_n.subs({n:i}) for i in range(0,8) ]
[oo, 1, 1/2, 1/3, 1/4,  1/5,   1/6,   1/7]  
>>> [ b_n.subs({n:i}) for i in range(0,8) ]
[1,  1, 1/2, 1/6, 1/24, 1/120, 1/720, 1/5040]
\end{verbatimtab}
\normalsize

\noindent
Observe that $a_n$ is not properly defined for $n=0$ since $\frac{1}{0}$ is a division-by-zero error.
To be precise, we should say $a_n$'s domain is the positive naturals $a_n:\mathbb{N}^+ \to \mathbb{R}$.
Observe how quickly the \texttt{factorial} function $n!=1\cdot2\cdot3\cdots(n-1)\cdot n$ grows:
$7!= 5040$, $10!=3628800$, $20! > 10^{18}$.



We're often interested in calculating the limits of sequences as $n\to \infty$.
What happens to the terms in the sequence when $n$ becomes large?

\small
\begin{verbatimtab}
>>> limit(a_n, n, oo)
0
>>> limit(b_n, n, oo)
0
\end{verbatimtab}
\normalsize

\noindent
Both $a_n=\frac{1}{n}$ and $b_n = \frac{1}{n!}$ \emph{converge} to $0$ as $n\to\infty$.							\index{convergence}

\medskip

Many important math quantities are defined as limit expressions.
An interesting example to consider is the number $\pi$,
which is defined as the area of a circle of radius $1$.	
We can approximate the area of the unit circle by drawing a many-sided regular polygon around the circle.
Splitting the $n$-sided regular polygon into identical triangular splices,
we can obtain a formula for its area $A_n$.
In the limit as $n\to \infty$, 
the $n$-sided-polygon approximation to the area of the unit-circle becomes exact:		\index{approximation}

\small
\begin{verbatimtab}
>>> A_n = n*tan(2*pi/(2*n))
>>> limit(A_n, n, oo)
pi
\end{verbatimtab}
\normalsize


\subsection{Series}
\label{calculus:series}

Suppose we're given a sequence $a_n$ and we want to compute the sum of all the values in this sequence $\sum_{n}^\infty a_n$.
Series are sums of sequences.																		\index{series}
Summing the values of a sequence $a_n:\mathbb{N}\to \mathbb{R}$
is analogous to taking the integral of a function $f:\mathbb{R}\to \mathbb{R}$.

To work with series in \texttt{SymPy},
use the \texttt{summation} function whose syntax is analogous to the \texttt{integrate} function: 						\index{summation}
																							\index{sequence!harmonic}

\small
\begin{verbatimtab}
>>> a_n = 1/n
>>> b_n = 1/factorial(n)
>>> summation(a_n, [n, 1, oo])
oo
>>> summation(b_n, [n, 0, oo])
E
\end{verbatimtab}
\normalsize

\noindent
We say the series $\sum a_n$ \emph{diverges} to infinity (or \emph{is divergent})								\index{divergence}
while the series $\sum b_n$ converges (or \emph{is convergent}).											\index{convergence}
As we sum together more and more terms of the sequence $b_n$, the total becomes closer and closer to some finite number.
In this case, the infinite sum $\sum_{n=0}^\infty \frac{1}{n!}$ converges to the number $e=2.71828\ldots$.


The \texttt{summation} command is useful because it allows us to compute \emph{infinite} sums,
but for most practical applications we don't need to take an infinite number of terms in a series to obtain a good approximation. 
This is why series are so neat: they represent a great way to obtain approximations.

Using standard Python commands,  
we can obtain an approximation to $e$ that is accurate to six decimals by summing 10 terms in the series: 

\small
\begin{verbatimtab}
>>> import math
>>> def b_nf(n): 
        return 1.0/math.factorial(n)
>>> sum( [b_nf(n) for n in range(0,10)] )
2.718281 52557319
>>> E.evalf()
2.718281 82845905       # true value
\end{verbatimtab}
\normalsize
\subsection{Taylor series}
\label{calculus:taylor_series}

Wait, there's more! 
Not only can we use series to approximate numbers,
we can also use them to approximate functions.

A \emph{power series} is a series whose terms contain different powers of the variable $x$.						\index{Taylor series}
The $n$\textsuperscript{th} term in a power series is a function of both the sequence index $n$ and the input variable $x$.

For example, the power series of the function $\exp(x)=e^x$ is 
\[
 \exp(x)	=	1 + x + \frac{x^2}{2} + \frac{x^3}{3!} + \frac{x^4}{4!} + \frac{x^5}{5!} + \cdots         
		= 	\sum_{n=0}^\infty \frac{x^n}{n!}.
\]
This is, IMHO, one of the most important ideas in calculus:
you can compute the value of $\exp(5)$ by taking the infinite sum of the terms in the power series with $x=5$:



\small
\begin{verbatimtab}
>>> exp_xn = x**n/factorial(n)
>>> summation( exp_xn.subs({x:5}), [n, 0, oo] ).evalf()
148.413159102577
>>> exp(5).evalf()
148.413159102577        # the true value
\end{verbatimtab}
\normalsize

\noindent
Note that \texttt{SymPy} is actually smart enough to recognize that the infinite series
you're computing corresponds to the closed-form expression $e^5$:



\small
\begin{verbatimtab}
>>> summation( exp_xn.subs({x:5}), [n, 0, oo])
exp(5)
\end{verbatimtab}
\normalsize
Taking as few as 35 terms in the series is sufficient to obtain an approximation to $e$
that is accurate to $16$ decimals:
%so series are not some abstract thing for mathematicians but a practical trick you can when you code:

\small
\begin{verbatimtab}
>>> import math                    # redo using only python 
>>> def exp_xnf(x,n): 
        return x**n/math.factorial(n)
>>> sum( [exp_xnf(5.0,i) for i in range(0,35)] )
148.413159102577
\end{verbatimtab}
\normalsize

\noindent
The coefficients in the power series of a function (also known as the \emph{Taylor series})
depend on the value of the higher derivatives of the function. 
The formula for the $n$\textsuperscript{th} term in the Taylor series of $f(x)$ expanded at $x=c$ is $a_n(x) = \frac{f^{(n)}(c)}{n!}(x-c)^n$,
where $f^{(n)}(c)$ is the value of the $n$\textsuperscript{th} derivative of $f(x)$ evaluated at $x=c$.
%The term \emph{Taylor series} applies to all series expansions of functions.
The term \emph{Maclaurin series} refers to Taylor series expansions at $x=0$.										\index{Maclaurin series}

The \texttt{SymPy} function \texttt{series} is a convenient way to obtain the series of any function.
Calling \texttt{series(expr,var,at,nmax)} 
will show you the series expansion of \texttt{expr} 
near \texttt{var}=\texttt{at} 
up to power \texttt{nmax}:

\small
\begin{verbatimtab}
>>> series( sin(x), x, 0, 8)
x - x**3/6 + x**5/120 - x**7/5040 + O(x**8)
>>> series( cos(x), x, 0, 8)
1 - x**2/2 + x**4/24 - x**6/720 + O(x**8)
>>> series( sinh(x), x, 0, 8)
x + x**3/6 + x**5/120 + x**7/5040 + O(x**8)
>>> series( cosh(x), x, 0, 8)
1 + x**2/2 + x**4/24 + x**6/720 + O(x**8)
\end{verbatimtab}
\normalsize

%Note the power series of $\sin$ and $\sinh$ contain only odd powers of $x$
%while the power series of $\cos$ and $\cosh$ contain only even powers.

\noindent
Some functions are not defined at $x=0$, so we expand them at a different value of $x$.
For example, the power series of $\ln(x)$ expanded at $x=1$ is

\small
\begin{verbatimtab}
>>> series(ln(x), x, 1, 6)     # Taylor series of ln(x) at x=1
x - x**2/2 + x**3/3 - x**4/4 + x**5/5  + O(x**6)    
\end{verbatimtab}
\normalsize

\noindent
Here, the result \texttt{SymPy} returns is misleading.
The Taylor series of $\ln(x)$ expanded at $x=1$ has terms of the form $(x-1)^n$:
\[
  \ln(x) = (x-1) - \frac{(x-1)^2}{2} + \frac{(x-1)^3}{3} - \frac{(x-1)^4}{4} + \frac{(x-1)^5}{5} + \cdots.
\]
Verify this is the correct formula by substituting $x=1$.
\texttt{SymPy} returns an answer in terms of coordinates \emph{relative} to $x=1$.
%That's okay, 
%because when dealing with series in general we're mostly interested in the coefficients.

Instead of expanding $\ln(x)$ around $x=1$,
we can obtain an equivalent expression if we expand $\ln(x+1)$ around $x=0$:



\small
\begin{verbatimtab}
>>> series(ln(x+1), x, 0, 6)   # Maclaurin series of ln(x+1)
x - x**2/2 + x**3/3 - x**4/4 + x**5/5 + O(x**6)
\end{verbatimtab}
\normalsize


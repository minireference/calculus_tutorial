%!TEX root = ../calculus_tutorial.tex

	\subsection{Applications of integration}

		Intuitively,
		we use integrals whenever we want to
		compute the ``total'' of some quantity that varies over time or space.

		\subsubsection{Kinematics}

    			Calculus was originally invented
			to describe the equations of motion $x(t)$, $v(t)$, and $a(t)$,
			which correspond to the object's \emph{position}, \emph{velocity},
			and \emph{acceleration} at time $t$.
			We call these the \emph{kinematics} equations,
			from the Greek word \emph{kinema} for motion.
			The velocity function $v(t)$ is the derivative of the position function,
			and the acceleration $a(t)$ is the derivative of the velocity,
			which we can summarize as follows:
			\[
				a(t) \overset{\frac{d}{dt} }{\longleftarrow} v(t) \overset{\frac{d}{dt} }{\longleftarrow} x(t).
			\]
			The starting point of kinematics is Newton's second law,
			which tells us that the acceleration of an object of mass $m$
			that has a net force $F_{\textrm{net}}$ acting on it is
			$a = \tfrac{1}{m}F_{\textrm{net}}$.
			Given the knowledge of acceleration over time $a(t)$,
			we can predict the position of the object $x(t)$ at any time $t$
			by ``undoing'' the derivative operations using integration:
			\[
				% \underbrace{ 
				  \tfrac{1}{m} F_{\textrm{net}}(t)
				% }_{\text{dynamics}}
				\; = \; 
				\underbrace{ 
				  a(t) \; \overset{v_i + \int\!dt }{\longrightarrow} \; 
				  v(t) \; \overset{x_i + \int\!dt }{\longrightarrow} \; 
				  x(t) 
				}_{\text{kinematics}}.
			\]
			We integrate $a(t)$ to obtain $v(t)$
			and choose the initial velocity $v_i$ as the integration constant so that $v(0) = v_i$.
			We then use integration a second time to obtain $x(t)$ from $v(t)$,
			using the initial position $x_i = x(0)$ as the integration constant.			

			The case of \emph{uniform accelerated motion} (UAM) is of particular interest.
			% since it describes the trajectory of objects falling under the effect of gravity.
			%	Consider an object of mass $m$ that has a constant net $F_{\textrm{net}}$
			%	acting on it will experience 
			Consider an object that experiences a constant acceleration $a(t) = a$.
			We can use integration to find the velocity of this object at a later time $t=\tau$:
			\[ 
				v(\tau)	= v_i + \int_0^\tau a(t)\,dt
						= v_i +  \int_0^\tau a \,dt
						= v_i + a\tau.
			\]
			% where $v_i$ is the initial velocity of the object at $t=0$.
			Knowing the velocity as a function of time $v(t)$,
			we can use integration a second time to find its position at time $\tau$:
			\[
				x(\tau)	= x_i + \int_0^\tau v(t) \, dt
						= x_i +  \int_0^\tau (v_i+at) \, dt
						= x_i + v_i\tau + \tfrac{1}{2}a\tau^2.
			\]
			These two simple calculus steps
			allow us to obtain the famous kinematics equation
			$x(t) = x_i + v_it + \tfrac{1}{2}at^2$
			for descibing the motion of objects undergoing constant acceleration $a$. % $a(t) = a$.
			Students taking a physics class are normally presented with this equation
			and it seems to come out of nowhere,
			but if you understand calculus you'll know where it comes from:
			the integration operation applied to the acceleration function $a(t) = a$
			and the initial conditions $x_i \eqdef x(0)$ and $v_i \eqdef v(0)$.

			% don't have memorize it,
			% you can always derive from first principles $a(t) = a$,
			%
			%	Note that the above calculations required knowing the initial conditions $x_i$ and $v_i$.
			%	These initial values were required
			%	because the integral calculations we performed
			%	only tell us the \emph{change} in the quantities relative to their initial values.


		\subsubsection{Solving differential equations}

			Many important laws in science % and engineering
			are described by \emph{differential equations}
			that specify an unknown function $f(t)$ in terms of their derivatives
			$f^{\prime\!}(t)$, $f^{\prime\prime\!}(t)$, etc.

			Here are some examples of differential equations and their solutions:

			\begin{itemize}

				\item	The kinematics equations when the acceleration is constant
					come from the differential equation $x^{\prime\prime\!}(t) = a$.
					We use integration twice to find the unknown function
					$x(t) = x_i + v_it + \tfrac{1}{2}at^2$.
					We can verify that $x(t)$ is a solution to the differential equation $x^{\prime\prime\!}(t) = a$
					by computing the second derivative of $x(t)$.
					% which is $x^{\prime\prime\!}(t) = a$.

				\item In biology,
					unconstrained bacterial growth is described by the equation $b^{\prime\!}(t) = kb(t)$,
					where $b(t)$ is the number of bacteria at time $t$.
					Intuitively,
					the bacterial growth rate $b^{\prime\!}(t)$
					is proportional to the number of existing bacteria.
					The solution to this equation is $b(t) = b_0e^{k t}$,
					where $b_0$ describes the number of bacteria at time $t = 0$.

				\item Radioactive decay is described by the differential equation
					$r^{\prime\!}(t) = -\lambda r(t)$,
					where $r(t)$ describes the number of atoms of some radioactive element.
					The solution is $r(t) = r_0e^{-\lambda t}$.

				\item Simple harmonic motion is described by the
					differential equation $x^{\prime\prime\!}(t) + \omega^2 x(t) = 0$,
					which has solution $x(t) = c_1 \sin(\omega t) + c_2 \cos(\omega t)$,
					for some constants $c_1$ and $c_2$.

			\end{itemize}

			\noindent
			If you take a course on differential equations,
			you'll learn all kind of tricks and techniques for solving differential equations.
			Integration plays a key role in all these techniques,
			since it allows us to ``undo'' the derivative operation.			
			%	We use integrals to solve differential equations.
			%	If we have to solve for $f(x)$ in the differential equation $\frac{d}{dx}f(x) = g(x)$,
			%	we can take the integral on both sides of the equation to obtain the answer $f(x) = \int g(x)\,dx + C$.


		\subsubsection{Probability calculations}
		% ALT. Probability and expectation calculations

			Integration is a key tool for computing probabilities of continuous random variables.
			A continuous random variable $X$ is described by its \emph{probability density function} $f_X$,
			and the probability of the event $\{ a \leq X \leq b\}$ is given by the integral
			$\Pr( \{ a \leq X \leq b\} ) \eqdef \int_a^b f_X(x) \, dx$.
			%	The probability density $f_X$ varies for different values of $x$,
			%	so if we want to compute the total probability of $X$ falling between $x=a$ and $x=b$,
			%	we must compute the integral of $f_X$.
			%
			For example,
			the standard normal random variable $Z$
			is described by the probability density function
			$f_Z(z) = \tfrac{1}{\sqrt{2\pi}} e^{ -\frac{1}{2}z^2 }$.
			To calculate the probability of the event $\{ -1 \leq Z \leq 1\}$,
			we must evaluate the integral $\int_{-1}^1 f_Z(z) \, dz$,
			which is easy to do using SciPy helper function $\tt{quad(}f_Z\tt{,a=-1,b=1)[0]} = 0.68269$.
			% MAYBE: this means 68\% of outcomes of the random variable Z will be between -1 and 1

			\ifthenelse{\boolean{FORSTATSBOOK}}{
				We also use integration to compute \emph{expectations}
				for quantities that depend on continuous random variables.
				The expected value of the quantity $G=g(X)$
				under the randomness of a continuous random variable $X$
				is defined as the following integral calculation:
				$\EE_X[g(X)] \eqdef \int_{x \in \calX} g(x)f_X(x)\,dx$.
				The expected value is computed by ``weighing'' each value of $g(x)$
				by the corresponding probability density for the event $\{X=x\}$,
				summed over all possible values for the random variable $X$.
				The mean $\mu = \EE_X[X]$ and the variance $\sigma^2 = \EE_X[(X-\mu)^2]$
				are two central concepts in probability theory and statistics
				that are computed as expectation integrals.
				Every time we use the $\EE_X$ notation in Section~\ref{sec:continuous_prob_distr},
				there is some integral calculation going on behind the scenes,
				so its good if you know to know a thing or two about integrals.
			}{}
